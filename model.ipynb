{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "significant-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "peaceful-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGNN_lp_layer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_metapaths_list,\n",
    "                 num_edge_type,\n",
    "                 etypes_lists,\n",
    "                 in_dim,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 attn_vec_dim,\n",
    "                 attn_drop=0.5):\n",
    "        super(MAGNN_lp_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        # ctr_ntype-specific layers\n",
    "        self.user_layer = MAGNN_ctr_ntype_specific(num_metapaths_list[0],\n",
    "                                                   etypes_lists[0],\n",
    "                                                   in_dim,\n",
    "                                                   num_heads,\n",
    "                                                   attn_vec_dim,\n",
    "                                                   attn_drop,\n",
    "                                                   use_minibatch=True)\n",
    "        self.item_layer = MAGNN_ctr_ntype_specific(num_metapaths_list[1],\n",
    "                                                   etypes_lists[1],\n",
    "                                                   in_dim,\n",
    "                                                   num_heads,\n",
    "                                                   attn_vec_dim,\n",
    "                                                   attn_drop,\n",
    "                                                   use_minibatch=True)\n",
    "\n",
    "        # note that the acutal input dimension should consider the number of heads\n",
    "        # as multiple head outputs are concatenated together\n",
    "        self.fc_user = nn.Linear(in_dim * num_heads, out_dim, bias=True)\n",
    "        self.fc_item = nn.Linear(in_dim * num_heads, out_dim, bias=True)\n",
    "        nn.init.xavier_normal_(self.fc_user.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.fc_item.weight, gain=1.414)\n",
    "    def get_loss(self):\n",
    "        loss=0\n",
    "        loss+=self.user_layer.get_loss()\n",
    "        loss+=self.item_layer.get_loss()\n",
    "        return loss\n",
    "    def forward(self, inputs):\n",
    "        g_lists, features, type_mask, edge_metapath_indices_lists, target_idx_lists,node_lists= inputs\n",
    "\n",
    "        # ctr_ntype-specific layers\n",
    "        h_user = self.user_layer(\n",
    "            (g_lists[0], features, type_mask, edge_metapath_indices_lists[0], target_idx_lists[0],node_lists[0],[1,2,3]))\n",
    "\n",
    "        h_item = self.item_layer(\n",
    "            (g_lists[1], features, type_mask, edge_metapath_indices_lists[1], target_idx_lists[1],node_lists[1],[4,5,6]))\n",
    "\n",
    "        logits_user = self.fc_user(h_user)\n",
    "        logits_item = self.fc_item(h_item)\n",
    "        return [logits_user, logits_item], [h_user, h_item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proved-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGNN_lp(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_metapaths_list,\n",
    "                 num_edge_type,\n",
    "                 etypes_lists,\n",
    "                 feats_dim_list,\n",
    "                 hidden_dim,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 attn_vec_dim,\n",
    "                 dropout_rate=0.5):\n",
    "        super(MAGNN_lp, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # ntype-specific transformation\n",
    "        self.fc_list = nn.ModuleList([nn.Linear(feats_dim, hidden_dim, bias=True) for feats_dim in feats_dim_list])\n",
    "        # feature dropout after trainsformation\n",
    "        if dropout_rate > 0:\n",
    "            self.feat_drop = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.feat_drop = lambda x: x\n",
    "        # initialization of fc layers\n",
    "        for fc in self.fc_list:\n",
    "            nn.init.xavier_normal_(fc.weight, gain=1.414)\n",
    "\n",
    "        # MAGNN_lp layers\n",
    "        self.layer1 = MAGNN_lp_layer(num_metapaths_list,\n",
    "                                     num_edge_type,\n",
    "                                     etypes_lists,\n",
    "                                     hidden_dim,\n",
    "                                     out_dim,\n",
    "                                     num_heads,\n",
    "                                     attn_vec_dim,\n",
    "                                     attn_drop=dropout_rate)\n",
    "    def get_loss(self):\n",
    "        return self.layer1.get_loss()\n",
    "    def forward(self, inputs):\n",
    "        g_lists, features_list, type_mask, edge_metapath_indices_lists, target_idx_lists,node_lists = inputs\n",
    "\n",
    "        # ntype-specific transformation\n",
    "        transformed_features = torch.zeros(type_mask.shape[0], self.hidden_dim, device=features_list[0].device)\n",
    "        for i, fc in enumerate(self.fc_list):\n",
    "            node_indices = np.where(type_mask == i)[0]\n",
    "            transformed_features[node_indices] = fc(features_list[i])\n",
    "        transformed_features = self.feat_drop(transformed_features)\n",
    "\n",
    "        # hidden layers\n",
    "        [logits_user, logits_item], [h_user, h_item] = self.layer1(\n",
    "            (g_lists, transformed_features, type_mask, edge_metapath_indices_lists, target_idx_lists,node_lists))\n",
    "\n",
    "        return [logits_user, logits_item], [h_user, h_item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-george",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:workspace] *",
   "language": "python",
   "name": "conda-env-workspace-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
