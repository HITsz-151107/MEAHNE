{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "average-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as  nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "great-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class aggregation_coder(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim):\n",
    "        super(aggregation_coder,self).__init__()\n",
    "        self.l1=nn.Linear(in_dim,hidden_dim)\n",
    "        self.l2=nn.Linear(hidden_dim,out_dim)\n",
    "        self.l3=nn.Linear(hidden_dim,out_dim)\n",
    "        self.l4=nn.Linear(hidden_dim,out_dim)\n",
    "        nn.init.xavier_normal_(self.l1.weight,gain=1.414)\n",
    "        nn.init.xavier_normal_(self.l2.weight,gain=1.414)\n",
    "    def forward(self,shuru):\n",
    "        out=self.l1(shuru)\n",
    "        out=F.leaky_relu(out)\n",
    "    #    out=F.leaky_relu(self.l2(out))\n",
    "     #   out=F.leaky_relu(self.l3(out))\n",
    "     #   out=F.leaky_relu(self.l4(out))\n",
    "     #   out=self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "affecting-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGNN_ctr_ntype_specific(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_metapaths,\n",
    "                 etypes_list,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 attn_vec_dim,\n",
    "                 attn_drop=0.5,\n",
    "                 use_minibatch=False):\n",
    "        super(MAGNN_ctr_ntype_specific, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_minibatch = use_minibatch\n",
    "        self.emb=[]\n",
    "        # metapath-specific layers\n",
    "        self.metapath_layers = nn.ModuleList()\n",
    "        self.aggregation_coder=aggregation_coder(out_dim*3,out_dim,out_dim)\n",
    "        for i in range(num_metapaths):\n",
    "            self.metapath_layers.append(MAGNN_metapath_specific(etypes_list[i],\n",
    "                                                                out_dim,\n",
    "                                                                num_heads,\n",
    "                                                                attn_drop=attn_drop,\n",
    "                                                                use_minibatch=use_minibatch))\n",
    "\n",
    "        # metapath-level attention\n",
    "        # note that the acutal input dimension should consider the number of heads\n",
    "        # as multiple head outputs are concatenated together\n",
    "        self.fc1 = nn.Linear(out_dim * num_heads, attn_vec_dim, bias=True)\n",
    "        self.fc2 = nn.Linear(attn_vec_dim, 1, bias=False)\n",
    "        self.drop10=nn.Linear(out_dim*num_heads,attn_vec_dim,bias=True)\n",
    "        self.drop11=nn.Linear(attn_vec_dim, 1, bias=False)\n",
    "        self.drop20=nn.Linear(out_dim*num_heads,attn_vec_dim,bias=True)\n",
    "        self.drop21=nn.Linear(attn_vec_dim, 1, bias=False)\n",
    "        self.drop30=nn.Linear(out_dim*num_heads,attn_vec_dim,bias=True)\n",
    "        self.drop31=nn.Linear(attn_vec_dim, 1, bias=False)\n",
    "        \n",
    "        \n",
    "        # weight initialization\n",
    "        nn.init.xavier_normal_(self.fc1.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.fc2.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop10.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop11.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop20.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop21.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop30.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop31.weight, gain=1.414)\n",
    "   \n",
    "    def forward(self, inputs):\n",
    "   \n",
    "        if self.use_minibatch:\n",
    "            g_list, features, type_mask, edge_metapath_indices_list, target_idx_list,node_list,num_list = inputs\n",
    "            # metapath-specific layers\n",
    "            metapath_outs = [F.elu(metapath_layer((g, features, type_mask, edge_metapath_indices, target_idx,nodes,num)).view(-1, self.num_heads * self.out_dim))\n",
    "                             for g, edge_metapath_indices, target_idx, nodes,num,metapath_layer in zip(g_list, edge_metapath_indices_list, target_idx_list,node_list,num_list,self.metapath_layers)]\n",
    "        else:\n",
    "            g_list, features, type_mask, edge_metapath_indices_list,node_list,num_list = inputs\n",
    "\n",
    "            # metapath-specific layers\n",
    "            metapath_outs = [F.elu(metapath_layer((g, features, type_mask, edge_metapath_indices)).view(-1, self.num_heads * self.out_dim))\n",
    "                             for g, edge_metapath_indices, metapath_layer in zip(g_list, edge_metapath_indices_list, self.metapath_layers)]\n",
    "        ceta=torch.cat(metapath_outs,dim=1)\n",
    "        h=self.aggregation_coder(ceta)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-advance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:workspace] *",
   "language": "python",
   "name": "conda-env-workspace-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
