{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threaded-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "composed-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "class edge_coder(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim,edge_drop):\n",
    "        super(edge_coder,self).__init__()\n",
    "        self.l1=nn.Linear(in_dim,hidden_dim)\n",
    "        self.l2=nn.Linear(hidden_dim,out_dim)\n",
    "        self.l3=nn.Linear(hidden_dim,out_dim)\n",
    "        nn.init.xavier_normal_(self.l1.weight,gain=1.414)\n",
    "    def forward(self,shuru):\n",
    "        out=self.l1(shuru)\n",
    "        out=F.relu(out)\n",
    "      #  out=F.relu(self.l2(out))\n",
    "    #    out=F.relu(self.l3(out))\n",
    "        #    out=self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unauthorized-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN1(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN1, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "        self.l1=nn.Linear(in_feats,out_feats)\n",
    "        self.l2=nn.Linear(in_feats,out_feats)\n",
    "    def message_func(self,edges):\n",
    "        return {'h':edges.src['h'],'a_drop':edges.data['a_drop'],'aft':edges.data['aft']}\n",
    "        \n",
    "    def reduce_func(self,nodes):\n",
    "        #alpha=F.softmax(nodes.mailbox['e'],dim=1)\n",
    "        alpha=nodes.mailbox['a_drop']\n",
    "  #      aft=nodes.mailbox['aft']\n",
    "     #   h=nodes.mailbox['h']\n",
    "       # aft=aft.view(-1,64)     \n",
    "      #  print(alpha.shape)\n",
    "  #      print(h.shape)\n",
    "  #      print(aft.shape)\n",
    "        z=torch.sum(alpha*nodes.mailbox['h']+F.leaky_relu(self.l1(nodes.mailbox['aft'])),dim=1)\n",
    "     #   z=F.leaky_relu(self.l2(z))\n",
    "     #   print(z.shape)\n",
    "     #   z+=aft\n",
    "        return {'z':z}\n",
    "    def forward(self, g, feature):\n",
    "  #      print(g)\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(self.message_func,self.reduce_func)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "\n",
    "        return g.ndata.pop('z')\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "        nn.init.xavier_normal_(self.linear.weight,gain=1.414)\n",
    "\n",
    "    def forward(self, node):\n",
    "        z= self.linear(node.data['z'])\n",
    "        if self.activation is not None:\n",
    "            z = self.activation(z)\n",
    "    #    z=F.dropout(z, p=0.5, training=self.training)    \n",
    "        return {'z' : z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "occasional-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,in_feat,out_feat,activation):\n",
    "        super(Net,self).__init__()\n",
    "        self.gcn1=GCN1(in_feat,out_feat,activation)\n",
    "        self.gcn2=GCN1(in_feat,out_feat,activation)\n",
    "        self.gcn3=GCN1(in_feat,out_feat,None)\n",
    "        self.l1=nn.Linear(in_feat,out_feat)\n",
    "        self.self_weight = nn.Parameter(torch.FloatTensor(in_feat, out_feat))\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.self_weight.size(1))\n",
    "        self.self_weight.data.uniform_(-stdv, stdv)    \n",
    "       # self.gcn4=GCN2(in_feat,out_feat,activation)\n",
    "       # self.gcn5=GCN2(in_feat,out_feat,None)\n",
    "    def forward(self,g,feature):\n",
    "        x=self.gcn1(g,feature)\n",
    "       # y=self.gcn2(g,x)\n",
    "        y=torch.mm(feature,self.self_weight)\n",
    "        x+=y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "underlying-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGNN_metapath_specific(nn.Module):\n",
    "    def __init__(self,\n",
    "                 etypes,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 attn_drop=0.5,\n",
    "                 alpha=0.01,\n",
    "                 use_minibatch=False,\n",
    "                 attn_switch=False):\n",
    "        super(MAGNN_metapath_specific, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "       # self.rnn_type = rnn_type\n",
    "        self.etypes = etypes\n",
    "        self.use_minibatch = use_minibatch\n",
    "        self.attn_switch = attn_switch\n",
    "        self.coder1=edge_coder(out_dim*3,out_dim,out_dim,0.5)\n",
    "        self.coder2=edge_coder(out_dim*5,out_dim,out_dim,0.5)\n",
    "        self.coder3=edge_coder(out_dim*4,out_dim,out_dim,0.5)\n",
    "        self.coder4=edge_coder(out_dim*3,out_dim,out_dim,0.5)\n",
    "        self.coder5=edge_coder(out_dim*3,out_dim,out_dim,0.5)\n",
    "        self.coder6=edge_coder(out_dim*4,out_dim,out_dim,0.5)\n",
    "        self.Net=Net(out_dim,out_dim,F.relu)\n",
    "       # self.Multi_Net=Multi_Net(out_dim,out_dim,out_dim)\n",
    "        # rnn-like metapath instance aggregator\n",
    "        # consider multiple attention heads\n",
    "        # node-level attention\n",
    "        # attention considers the center node embedding or not\n",
    "        if self.attn_switch:\n",
    "            self.attn1 = nn.Linear(out_dim, num_heads, bias=False)\n",
    "            self.attn2 = nn.Parameter(torch.empty(size=(1, num_heads, out_dim)))\n",
    "        else:\n",
    "            self.attn = nn.Parameter(torch.empty(size=(1, num_heads, out_dim)))\n",
    "        self.leaky_relu = nn.LeakyReLU(alpha)\n",
    "        self.softmax = edge_softmax\n",
    "        if attn_drop:\n",
    "            self.attn_drop = nn.Dropout(attn_drop)\n",
    "        else:\n",
    "            self.attn_drop = lambda x: x\n",
    "\n",
    "        # weight initialization\n",
    "        if self.attn_switch:\n",
    "            nn.init.xavier_normal_(self.attn1.weight, gain=1.414)\n",
    "            nn.init.xavier_normal_(self.attn2.data, gain=1.414)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(self.attn.data, gain=1.414)\n",
    "\n",
    "    def edge_softmax(self, g):\n",
    "        attention = self.softmax(g, g.edata.pop('a'))\n",
    "        # Dropout attention scores and save them\n",
    "        g.edata['a_drop'] = self.attn_drop(attention)\n",
    "\n",
    "    def message_passing(self, edges):\n",
    "        ft = edges.data['aft'] * edges.data['a_drop']\n",
    "        return {'ft': ft}\n",
    "    def message_pass(self,edges):\n",
    "        return {'h':edges.src['h'],'a':edges.data['a']}\n",
    "    def reduce_func(self,nodes):\n",
    "        alpha=F.softmax(nodes.mailbox['a'],dim=1)\n",
    "        alpha_drop=self.attn_drop(alpha)\n",
    "        z=torch.sum(alpha_drop*nodes.mailbox['h'],dim=1)\n",
    "        return {'z':z}\n",
    "    def forward(self, inputs):\n",
    "        # features: num_all_nodes x out_dim\n",
    "        if self.use_minibatch:\n",
    "            g, features, type_mask, edge_metapath_indices, target_idx,nodes,num = inputs\n",
    "        else:\n",
    "            g, features, type_mask, edge_metapath_indices,nodes,num= inputs\n",
    "     #   print(nodes)\n",
    "   #     print(g)\n",
    "     #   print(len(nodes))\n",
    "        g_feat=F.embedding(nodes,features)\n",
    "      #  print(g_feat.shape)\n",
    "    #    g.ndata['h']=g_feat\n",
    "    #    print(g_feat)\n",
    "        # Embedding layer\n",
    "        # use torch.nn.functional.embedding or torch.embedding here\n",
    "        # do not use torch.nn.embedding\n",
    "        # edata: E x Seq x out_dim\n",
    "        \n",
    "        edata = F.embedding(edge_metapath_indices, features)\n",
    "       # print(edata.shape)\n",
    "        \n",
    "        fdata=edata.reshape(edata.shape[0],-1)\n",
    "     #   print(fdata.shape)\n",
    "        if(num==1):\n",
    "            fdata=self.coder1(fdata)\n",
    "        if(num==2):\n",
    "            fdata=self.coder2(fdata)\n",
    "        if(num==3):\n",
    "            fdata=self.coder3(fdata)\n",
    "        if(num==4):\n",
    "            fdata=self.coder4(fdata)\n",
    "        if(num==5):\n",
    "            fdata=self.coder5(fdata)\n",
    "        if(num==6):\n",
    "            fdata=self.coder6(fdata)\n",
    "        \n",
    "        #eft = hidden.permute(1, 0, 2).view(-1, self.num_heads, self.out_dim)  # E x num_heads x out_dim\n",
    "   #     print(self.attn.shape)\n",
    "    #    print(fdata.shape)\n",
    "        aft=fdata.unsqueeze(1)\n",
    "   #     print(aft.shape)\n",
    "        a = (aft * self.attn).sum(dim=-1)\n",
    "       \n",
    "       # a = (aft * self.attn).sum(dim=-1).unsqueeze(dim=-1)  # E x num_heads x 1\n",
    "   #     print('ashape')\n",
    "   #     print(a.shape)    \n",
    "     #   a = self.leaky_relu(a)\n",
    "        aft=aft.squeeze(dim=1)\n",
    "    #    print(aft.shape)\n",
    "        g.edata.update({'aft': aft, 'a': a})\n",
    "        # compute softmax normalized attention values\n",
    "        self.edge_softmax(g)\n",
    "        # compute the aggregated node features scaled by the dropped,\n",
    "        # unnormalized attention values.\n",
    "        #g.update_all(self.message_passing, fn.sum('ft', 'ft'))\n",
    " #       g.update_all(self.message_pass,self.reduce_func)\n",
    "  #      ret=g.ndata.pop('z')\n",
    "   #     ret = g.ndata.pop('ft')  # E x num_heads x out_dim\n",
    "        ret=self.Net(g,g_feat)\n",
    "        if self.use_minibatch:\n",
    "            return ret[target_idx]\n",
    "        else:\n",
    "            return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-parts",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:workspace] *",
   "language": "python",
   "name": "conda-env-workspace-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
