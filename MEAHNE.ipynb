{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "protected-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn.pytorch import edge_softmax\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integrated-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "class edge_coder(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim,edge_drop):\n",
    "        super(edge_coder,self).__init__()\n",
    "        self.l1=nn.Linear(in_dim,hidden_dim)\n",
    "        self.l2=nn.Linear(hidden_dim,out_dim)\n",
    "        self.l3=nn.Linear(hidden_dim,out_dim)\n",
    "        self.l4=nn.Linear(hidden_dim,out_dim)\n",
    "        nn.init.xavier_normal_(self.l1.weight,gain=1.414)\n",
    "    def forward(self,shuru):\n",
    "        out=self.l1(shuru)\n",
    "        out=F.relu(out)\n",
    "   #     out=F.relu(self.l2(out))\n",
    "     #   out=F.relu(self.l3(out))\n",
    "     #   out=F.relu(self.l4(out))\n",
    "    #    out=self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "determined-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "class aggregation_coder(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim):\n",
    "        super(aggregation_coder,self).__init__()\n",
    "        self.l1=nn.Linear(in_dim,hidden_dim)\n",
    "        self.l2=nn.Linear(hidden_dim,out_dim)\n",
    "        self.l3=nn.Linear(hidden_dim,out_dim)\n",
    "        self.l4=nn.Linear(hidden_dim,out_dim)\n",
    "        nn.init.xavier_normal_(self.l1.weight,gain=1.414)\n",
    "        nn.init.xavier_normal_(self.l2.weight,gain=1.414)\n",
    "    def forward(self,shuru):\n",
    "        out=self.l1(shuru)\n",
    "        out=F.leaky_relu(out)\n",
    "     #   out=F.leaky_relu(self.l2(out))\n",
    "     #   out=F.leaky_relu(self.l3(out))\n",
    "   #     out=F.leaky_relu(self.l4(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "banned-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN1(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN1, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "        self.l1=nn.Linear(in_feats,out_feats)\n",
    "        self.l2=nn.Linear(in_feats,out_feats)\n",
    "    def message_func(self,edges):\n",
    "        return {'h':edges.src['h'],'a_drop':edges.data['a_drop'],'aft':edges.data['aft']}\n",
    "        \n",
    "    def reduce_func(self,nodes):\n",
    "        #alpha=F.softmax(nodes.mailbox['e'],dim=1)\n",
    "        alpha=nodes.mailbox['a_drop']\n",
    "  #      aft=nodes.mailbox['aft']\n",
    "     #   h=nodes.mailbox['h']\n",
    "       # aft=aft.view(-1,64)     \n",
    "      #  print(alpha.shape)\n",
    "  #      print(h.shape)\n",
    "  #      print(aft.shape)\n",
    "        z=torch.sum(alpha*nodes.mailbox['h']+F.leaky_relu(self.l1(nodes.mailbox['aft'])),dim=1)\n",
    "     #   z=F.leaky_relu(self.l2(z))\n",
    "     #   print(z.shape)\n",
    "     #   z+=aft\n",
    "        return {'z':z}\n",
    "    def forward(self, g, feature):\n",
    "  #      print(g)\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(self.message_func,self.reduce_func)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "\n",
    "        return g.ndata.pop('z')\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "        nn.init.xavier_normal_(self.linear.weight,gain=1.414)\n",
    "\n",
    "    def forward(self, node):\n",
    "        z= self.linear(node.data['z'])\n",
    "        if self.activation is not None:\n",
    "            z = self.activation(z)\n",
    "    #    z=F.dropout(z, p=0.5, training=self.training)    \n",
    "        return {'z' : z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lined-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,in_feat,out_feat,activation):\n",
    "        super(Net,self).__init__()\n",
    "        self.gcn1=GCN1(in_feat,out_feat,activation)\n",
    "        self.gcn2=GCN1(in_feat,out_feat,activation)\n",
    "        self.gcn3=GCN1(in_feat,out_feat,None)\n",
    "        self.l1=nn.Linear(in_feat,out_feat)\n",
    "        self.self_weight = nn.Parameter(torch.FloatTensor(in_feat, out_feat))\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.self_weight.size(1))\n",
    "        self.self_weight.data.uniform_(-stdv, stdv)    \n",
    "       # self.gcn4=GCN2(in_feat,out_feat,activation)\n",
    "       # self.gcn5=GCN2(in_feat,out_feat,None)\n",
    "    def forward(self,g,feature):\n",
    "        x=self.gcn1(g,feature)\n",
    "    #    x=self.gcn2(g,x)\n",
    "        y=torch.mm(feature,self.self_weight)\n",
    "        x+=y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "viral-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEAHNE_metapath_specific(nn.Module):\n",
    "    def __init__(self,\n",
    "                 etypes,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 attn_drop=0.5,\n",
    "                 alpha=0.01,\n",
    "                 use_minibatch=False,\n",
    "                 attn_switch=False):\n",
    "        super(MEAHNE_metapath_specific, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "       # self.rnn_type = rnn_type\n",
    "        self.etypes = etypes\n",
    "        self.use_minibatch = use_minibatch\n",
    "        self.attn_switch = attn_switch\n",
    "        self.coder1=edge_coder(out_dim*3,out_dim,out_dim,0.5)\n",
    "        self.coder2=edge_coder(out_dim*5,out_dim,out_dim,0.5)\n",
    "        self.coder3=edge_coder(out_dim*4,out_dim,out_dim,0.5)\n",
    "        self.coder4=edge_coder(out_dim*3,out_dim,out_dim,0.5)\n",
    "        self.coder5=edge_coder(out_dim*3,out_dim,out_dim,0.5)\n",
    "        self.coder6=edge_coder(out_dim*4,out_dim,out_dim,0.5)\n",
    "        self.Net=Net(out_dim,out_dim,F.relu)\n",
    "       # self.Multi_Net=Multi_Net(out_dim,out_dim,out_dim)\n",
    "        # rnn-like metapath instance aggregator\n",
    "        # consider multiple attention heads\n",
    "        # node-level attention\n",
    "        # attention considers the center node embedding or not\n",
    "        if self.attn_switch:\n",
    "            self.attn1 = nn.Linear(out_dim, num_heads, bias=False)\n",
    "            self.attn2 = nn.Parameter(torch.empty(size=(1, num_heads, out_dim)))\n",
    "        else:\n",
    "            self.attn = nn.Parameter(torch.empty(size=(1, num_heads, out_dim)))\n",
    "        self.leaky_relu = nn.LeakyReLU(alpha)\n",
    "        self.softmax = edge_softmax\n",
    "        if attn_drop:\n",
    "            self.attn_drop = nn.Dropout(attn_drop)\n",
    "        else:\n",
    "            self.attn_drop = lambda x: x\n",
    "\n",
    "        # weight initialization\n",
    "        if self.attn_switch:\n",
    "            nn.init.xavier_normal_(self.attn1.weight, gain=1.414)\n",
    "            nn.init.xavier_normal_(self.attn2.data, gain=1.414)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(self.attn.data, gain=1.414)\n",
    "\n",
    "    def edge_softmax(self, g):\n",
    "        attention = self.softmax(g, g.edata.pop('a'))\n",
    "        # Dropout attention scores and save them\n",
    "        g.edata['a_drop'] = self.attn_drop(attention)\n",
    "\n",
    "    def message_passing(self, edges):\n",
    "        ft = edges.data['aft'] * edges.data['a_drop']\n",
    "        return {'ft': ft}\n",
    "    def message_pass(self,edges):\n",
    "        return {'h':edges.src['h'],'a':edges.data['a']}\n",
    "    def reduce_func(self,nodes):\n",
    "        alpha=F.softmax(nodes.mailbox['a'],dim=1)\n",
    "        alpha_drop=self.attn_drop(alpha)\n",
    "        z=torch.sum(alpha_drop*nodes.mailbox['h'],dim=1)\n",
    "        return {'z':z}\n",
    "    def forward(self, inputs):\n",
    "        # features: num_all_nodes x out_dim\n",
    "        if self.use_minibatch:\n",
    "            g, features, type_mask, edge_metapath_indices, target_idx,nodes,num = inputs\n",
    "        else:\n",
    "            g, features, type_mask, edge_metapath_indices,nodes,num= inputs\n",
    "     #   print(nodes)\n",
    "   #     print(g)\n",
    "     #   print(len(nodes))\n",
    "        g_feat=F.embedding(nodes,features)\n",
    "      #  print(g_feat.shape)\n",
    "    #    g.ndata['h']=g_feat\n",
    "    #    print(g_feat)\n",
    "        # Embedding layer\n",
    "        # use torch.nn.functional.embedding or torch.embedding here\n",
    "        # do not use torch.nn.embedding\n",
    "        # edata: E x Seq x out_dim\n",
    "        \n",
    "        edata = F.embedding(edge_metapath_indices, features)\n",
    "       # print(edata.shape)\n",
    "        \n",
    "        fdata=edata.reshape(edata.shape[0],-1)\n",
    "     #   print(fdata.shape)\n",
    "        if(num==1):\n",
    "            fdata=self.coder1(fdata)\n",
    "        if(num==2):\n",
    "            fdata=self.coder2(fdata)\n",
    "        if(num==3):\n",
    "            fdata=self.coder3(fdata)\n",
    "        if(num==4):\n",
    "            fdata=self.coder4(fdata)\n",
    "        if(num==5):\n",
    "            fdata=self.coder5(fdata)\n",
    "        if(num==6):\n",
    "            fdata=self.coder6(fdata)\n",
    "        \n",
    "        #eft = hidden.permute(1, 0, 2).view(-1, self.num_heads, self.out_dim)  # E x num_heads x out_dim\n",
    "   #     print(self.attn.shape)\n",
    "    #    print(fdata.shape)\n",
    "        aft=fdata.unsqueeze(1)\n",
    "   #     print(aft.shape)\n",
    "        a = (aft * self.attn).sum(dim=-1)\n",
    "       \n",
    "       # a = (aft * self.attn).sum(dim=-1).unsqueeze(dim=-1)  # E x num_heads x 1\n",
    "   #     print('ashape')\n",
    "   #     print(a.shape)    \n",
    "     #   a = self.leaky_relu(a)\n",
    "        aft=aft.squeeze(dim=1)\n",
    "    #    print(aft.shape)\n",
    "        g.edata.update({'aft': aft, 'a': a})\n",
    "        # compute softmax normalized attention values\n",
    "        self.edge_softmax(g)\n",
    "        # compute the aggregated node features scaled by the dropped,\n",
    "        # unnormalized attention values.\n",
    "        #g.update_all(self.message_passing, fn.sum('ft', 'ft'))\n",
    " #       g.update_all(self.message_pass,self.reduce_func)\n",
    "  #      ret=g.ndata.pop('z')\n",
    "   #     ret = g.ndata.pop('ft')  # E x num_heads x out_dim\n",
    "        ret=self.Net(g,g_feat)\n",
    "        if self.use_minibatch:\n",
    "            return ret[target_idx]\n",
    "        else:\n",
    "            return ret\n",
    "\n",
    "\n",
    "class MEAHNE_ctr_ntype_specific(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_metapaths,\n",
    "                 etypes_list,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 attn_vec_dim,\n",
    "                 attn_drop=0.5,\n",
    "                 use_minibatch=False):\n",
    "        super(MEAHNE_ctr_ntype_specific, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_minibatch = use_minibatch\n",
    "        self.emb=[]\n",
    "        # metapath-specific layers\n",
    "        self.metapath_layers = nn.ModuleList()\n",
    "        self.aggregation_coder=aggregation_coder(out_dim*3,out_dim,out_dim)\n",
    "        for i in range(num_metapaths):\n",
    "            self.metapath_layers.append(MEAHNE_metapath_specific(etypes_list[i],\n",
    "                                                                out_dim,\n",
    "                                                                num_heads,\n",
    "                                                                attn_drop=attn_drop,\n",
    "                                                                use_minibatch=use_minibatch))\n",
    "\n",
    "        # metapath-level attention\n",
    "        # note that the acutal input dimension should consider the number of heads\n",
    "        # as multiple head outputs are concatenated together\n",
    "        self.fc1 = nn.Linear(out_dim * num_heads, attn_vec_dim, bias=True)\n",
    "        self.fc2 = nn.Linear(attn_vec_dim, 1, bias=False)\n",
    "        self.drop10=nn.Linear(out_dim*num_heads,attn_vec_dim,bias=True)\n",
    "        self.drop11=nn.Linear(attn_vec_dim, 1, bias=False)\n",
    "        self.drop20=nn.Linear(out_dim*num_heads,attn_vec_dim,bias=True)\n",
    "        self.drop21=nn.Linear(attn_vec_dim, 1, bias=False)\n",
    "        self.drop30=nn.Linear(out_dim*num_heads,attn_vec_dim,bias=True)\n",
    "        self.drop31=nn.Linear(attn_vec_dim, 1, bias=False)\n",
    "        \n",
    "        \n",
    "        # weight initialization\n",
    "        nn.init.xavier_normal_(self.fc1.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.fc2.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop10.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop11.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop20.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop21.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop30.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.drop31.weight, gain=1.414)\n",
    "        \n",
    "    def get_loss(self):\n",
    "        loss=0\n",
    "        emb1=self.emb[0]\n",
    "        emb2=self.emb[1]\n",
    "        emb3=self.emb[2]\n",
    "        loss+=torch.mm(emb1,emb2.T)\n",
    "        loss+=torch.mm(emb1,emb3.T)\n",
    "        loss+=torch.mm(emb2,emb3.T)\n",
    "        return loss \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "   \n",
    "        if self.use_minibatch:\n",
    "            g_list, features, type_mask, edge_metapath_indices_list, target_idx_list,node_list,num_list = inputs\n",
    "            # metapath-specific layers\n",
    "            metapath_outs = [F.elu(metapath_layer((g, features, type_mask, edge_metapath_indices, target_idx,nodes,num)).view(-1, self.num_heads * self.out_dim))\n",
    "                             for g, edge_metapath_indices, target_idx, nodes,num,metapath_layer in zip(g_list, edge_metapath_indices_list, target_idx_list,node_list,num_list,self.metapath_layers)]\n",
    "        else:\n",
    "            g_list, features, type_mask, edge_metapath_indices_list,node_list,num_list = inputs\n",
    "\n",
    "            # metapath-specific layers\n",
    "            metapath_outs = [F.elu(metapath_layer((g, features, type_mask, edge_metapath_indices)).view(-1, self.num_heads * self.out_dim))\n",
    "                             for g, edge_metapath_indices, metapath_layer in zip(g_list, edge_metapath_indices_list, self.metapath_layers)]\n",
    "        ceta=torch.cat(metapath_outs,dim=1)\n",
    "        h=self.aggregation_coder(ceta)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "grave-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEAHNE_lp_layer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_metapaths_list,\n",
    "                 num_edge_type,\n",
    "                 etypes_lists,\n",
    "                 in_dim,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 attn_vec_dim,\n",
    "                 attn_drop=0.5):\n",
    "        super(MEAHNE_lp_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        # ctr_ntype-specific layers\n",
    "        self.user_layer = MEAHNE_ctr_ntype_specific(num_metapaths_list[0],\n",
    "                                                   etypes_lists[0],\n",
    "                                                   in_dim,\n",
    "                                                   num_heads,\n",
    "                                                   attn_vec_dim,\n",
    "                                                   attn_drop,\n",
    "                                                   use_minibatch=True)\n",
    "        self.item_layer = MEAHNE_ctr_ntype_specific(num_metapaths_list[1],\n",
    "                                                   etypes_lists[1],\n",
    "                                                   in_dim,\n",
    "                                                   num_heads,\n",
    "                                                   attn_vec_dim,\n",
    "                                                   attn_drop,\n",
    "                                                   use_minibatch=True)\n",
    "\n",
    "        # note that the acutal input dimension should consider the number of heads\n",
    "        # as multiple head outputs are concatenated together\n",
    "        self.fc_user = nn.Linear(in_dim * num_heads, out_dim, bias=True)\n",
    "        self.fc_item = nn.Linear(in_dim * num_heads, out_dim, bias=True)\n",
    "        nn.init.xavier_normal_(self.fc_user.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.fc_item.weight, gain=1.414)\n",
    "    def get_loss(self):\n",
    "        loss=0\n",
    "        loss+=self.user_layer.get_loss()\n",
    "        loss+=self.item_layer.get_loss()\n",
    "        return loss\n",
    "    def forward(self, inputs):\n",
    "        g_lists, features, type_mask, edge_metapath_indices_lists, target_idx_lists,node_lists= inputs\n",
    "\n",
    "        # ctr_ntype-specific layers\n",
    "        h_user = self.user_layer(\n",
    "            (g_lists[0], features, type_mask, edge_metapath_indices_lists[0], target_idx_lists[0],node_lists[0],[1,2,3]))\n",
    "\n",
    "        h_item = self.item_layer(\n",
    "            (g_lists[1], features, type_mask, edge_metapath_indices_lists[1], target_idx_lists[1],node_lists[1],[4,5,6]))\n",
    "\n",
    "        logits_user = self.fc_user(h_user)\n",
    "        logits_item = self.fc_item(h_item)\n",
    "        return [logits_user, logits_item], [h_user, h_item]\n",
    "\n",
    "\n",
    "class MEAHNE_lp(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_metapaths_list,\n",
    "                 num_edge_type,\n",
    "                 etypes_lists,\n",
    "                 feats_dim_list,\n",
    "                 hidden_dim,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 attn_vec_dim,\n",
    "                 dropout_rate=0.5):\n",
    "        super(MEAHNE_lp, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # ntype-specific transformation\n",
    "        self.fc_list = nn.ModuleList([nn.Linear(feats_dim, hidden_dim, bias=True) for feats_dim in feats_dim_list])\n",
    "        # feature dropout after trainsformation\n",
    "        if dropout_rate > 0:\n",
    "            self.feat_drop = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.feat_drop = lambda x: x\n",
    "        # initialization of fc layers\n",
    "        for fc in self.fc_list:\n",
    "            nn.init.xavier_normal_(fc.weight, gain=1.414)\n",
    "\n",
    "        # MEAHNE_lp layers\n",
    "        self.layer1 = MEAHNE_lp_layer(num_metapaths_list,\n",
    "                                     num_edge_type,\n",
    "                                     etypes_lists,\n",
    "                                     hidden_dim,\n",
    "                                     out_dim,\n",
    "                                     num_heads,\n",
    "                                     attn_vec_dim,\n",
    "                                     attn_drop=dropout_rate)\n",
    "    def get_loss(self):\n",
    "        return self.layer1.get_loss()\n",
    "    def forward(self, inputs):\n",
    "        g_lists, features_list, type_mask, edge_metapath_indices_lists, target_idx_lists,node_lists = inputs\n",
    "\n",
    "        # ntype-specific transformation\n",
    "        transformed_features = torch.zeros(type_mask.shape[0], self.hidden_dim, device=features_list[0].device)\n",
    "        for i, fc in enumerate(self.fc_list):\n",
    "            node_indices = np.where(type_mask == i)[0]\n",
    "            transformed_features[node_indices] = fc(features_list[i])\n",
    "        transformed_features = self.feat_drop(transformed_features)\n",
    "\n",
    "        # hidden layers\n",
    "        [logits_user, logits_item], [h_user, h_item] = self.layer1(\n",
    "            (g_lists, transformed_features, type_mask, edge_metapath_indices_lists, target_idx_lists,node_lists))\n",
    "\n",
    "        return [logits_user, logits_item], [h_user, h_item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience, verbose=False, delta=0, save_path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score - self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            with open(f,\"a\") as file:\n",
    "                file.write('Validation loss decreased :'+str(self.val_loss_min)+'-->'+str(val_loss)+'Saving model ...'+\"\\n\") \n",
    "        torch.save(model.state_dict(), self.save_path)\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lonely-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_LastFM_data(prefix='guoqing_data/'):\n",
    "    in_file = open(prefix + '0/0-1-0.adjlist', 'r')\n",
    "    adjlist00 = [line.strip() for line in in_file]\n",
    "    adjlist00 = adjlist00\n",
    "    in_file.close()\n",
    "    in_file = open(prefix + '0/0-1-2-1-0.adjlist', 'r')\n",
    "    adjlist01 = [line.strip() for line in in_file]\n",
    "    adjlist01 = adjlist01\n",
    "    in_file.close()\n",
    "    in_file = open(prefix + '0/0-2-2-0.adjlist', 'r')\n",
    "    adjlist02 = [line.strip() for line in in_file]\n",
    "    adjlist02 = adjlist02\n",
    "    in_file.close()\n",
    "    in_file = open(prefix + '1/1-0-1.adjlist', 'r')\n",
    "    adjlist10 = [line.strip() for line in in_file]\n",
    "    adjlist10 = adjlist10\n",
    "    in_file.close()\n",
    "    in_file = open(prefix + '1/1-2-1.adjlist', 'r')\n",
    "    adjlist11 = [line.strip() for line in in_file]\n",
    "    adjlist11 = adjlist11\n",
    "    in_file.close()\n",
    "    in_file = open(prefix + '1/1-2-2-1.adjlist', 'r')\n",
    "    adjlist12 = [line.strip() for line in in_file]\n",
    "    adjlist12 = adjlist12\n",
    "    in_file.close()\n",
    "\n",
    "    in_file = open(prefix + '0/0-1-0_idx.pickle', 'rb')\n",
    "    idx00 = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "    in_file = open(prefix + '0/0-1-2-1-0_idx.pickle', 'rb')\n",
    "    idx01 = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "    in_file = open(prefix + '0/0-2-2-0_idx.pickle', 'rb')\n",
    "    idx02 = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "    in_file = open(prefix + '1/1-0-1_idx.pickle', 'rb')\n",
    "    idx10 = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "    in_file = open(prefix + '1/1-2-1_idx.pickle', 'rb')\n",
    "    idx11 = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "    in_file = open(prefix + '1/1-2-2-1_idx.pickle', 'rb')\n",
    "    idx12 = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "\n",
    "    adjM = scipy.sparse.load_npz(prefix + 'adjM.npz')\n",
    "    type_mask = np.load(prefix + 'node_types.npy')\n",
    "    train_val_test_pos_mir_disease = np.load(prefix + '73clear_train_val_test_pos_mir_disease.npz')\n",
    "    train_val_test_neg_mir_disease = np.load(prefix + '73clear_train_val_test_neg_mir_disease.npz')\n",
    "\n",
    "    return [[adjlist00, adjlist01,adjlist02],[adjlist10, adjlist11,adjlist12]],\\\n",
    "           [[idx00, idx01,idx02], [idx10, idx11,idx12]],\\\n",
    "           adjM, type_mask, train_val_test_pos_mir_disease, train_val_test_neg_mir_disease\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "filled-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_adjlist_LastFM(adjlist, edge_metapath_indices, samples=None, exclude=None, offset=None, mode=None):\n",
    "    edges = []\n",
    "    nodes = set()\n",
    "    result_indices = []\n",
    "    #print(offset)\n",
    "    #print(exclude)\n",
    "    for row, indices in zip(adjlist, edge_metapath_indices):\n",
    "        row_parsed = list(map(int, row.split(' ')))\n",
    "        nodes.add(row_parsed[0])\n",
    "        if len(row_parsed) > 1:\n",
    "            # sampling neighbors\n",
    "            if samples is None:\n",
    "                if exclude is not None:\n",
    "                    if mode == 0:\n",
    "                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for u1, a1, u2, a2 in indices[:, [0, 1, -1, -2]]]\n",
    "                    else:\n",
    "                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for a1, u1, a2, u2 in indices[:, [0, 1, -1, -2]]]\n",
    "                    neighbors = np.array(row_parsed[1:])[mask]\n",
    "                    result_indices.append(indices[mask])\n",
    "                else:\n",
    "                    neighbors = row_parsed[1:]\n",
    "                    result_indices.append(indices)\n",
    "            else:\n",
    "                # undersampling frequent neighbors\n",
    "                unique, counts = np.unique(row_parsed[1:], return_counts=True)\n",
    "                p = []\n",
    "                for count in counts:\n",
    "                    p += [(count ** (3 / 4)) / count] * count\n",
    "                p = np.array(p)\n",
    "                p = p / p.sum()\n",
    "                samples = min(samples, len(row_parsed) - 1)\n",
    "                sampled_idx = np.sort(np.random.choice(len(row_parsed) - 1, samples, replace=False, p=p))\n",
    "                if exclude is not None:\n",
    "                    if mode == 0:\n",
    "                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for u1, a1, u2, a2 in indices[sampled_idx][:, [0, 1, -1, -2]]]\n",
    "                    else:\n",
    "                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for a1, u1, a2, u2 in indices[sampled_idx][:, [0, 1, -1, -2]]]\n",
    "                    neighbors = np.array([row_parsed[i + 1] for i in sampled_idx])[mask]\n",
    "                    result_indices.append(indices[sampled_idx][mask])\n",
    "                else:\n",
    "                    neighbors = [row_parsed[i + 1] for i in sampled_idx]\n",
    "                    result_indices.append(indices[sampled_idx])\n",
    "        else:\n",
    "            neighbors = [row_parsed[0]]\n",
    "            indices = np.array([[row_parsed[0]] * indices.shape[1]])\n",
    "            if mode == 1:\n",
    "                indices += offset\n",
    "            result_indices.append(indices)\n",
    "        for dst in neighbors:\n",
    "            nodes.add(dst)\n",
    "            edges.append((row_parsed[0], dst))\n",
    "    nodes=sorted(nodes)    \n",
    "   # print(nodes)\n",
    "    if mode==1:\n",
    "        node_list=[offset+i for i in nodes]\n",
    "    else:\n",
    "        node_list=[i for i in nodes]\n",
    " #   print(node_list)    \n",
    "    mapping = {map_from: map_to for map_to, map_from in enumerate(sorted(nodes))}\n",
    "    edges = list(map(lambda tup: (mapping[tup[0]], mapping[tup[1]]), edges))\n",
    "    result_indices = np.vstack(result_indices)\n",
    "    return edges, result_indices, len(nodes), mapping,node_list\n",
    "\n",
    "\n",
    "def parse_minibatch_LastFM(adjlists_ua, edge_metapath_indices_list_ua, user_artist_batch, device, samples=None, use_masks=None, offset=None):\n",
    "    g_lists = [[], []]\n",
    "    result_indices_lists = [[], []]\n",
    "    idx_batch_mapped_lists = [[], []]\n",
    "    node_list=[[],[]]\n",
    "    for mode, (adjlists, edge_metapath_indices_list) in enumerate(zip(adjlists_ua, edge_metapath_indices_list_ua)):\n",
    "        for adjlist, indices, use_mask in zip(adjlists, edge_metapath_indices_list, use_masks[mode]):\n",
    "     #       print(offset)\n",
    "            if use_mask:\n",
    "                edges, result_indices, num_nodes, mapping,nodes = parse_adjlist_LastFM(\n",
    "                    [adjlist[row[mode]] for row in user_artist_batch], [indices[row[mode]] for row in user_artist_batch], samples, user_artist_batch, offset, mode)\n",
    "            else:\n",
    "                edges, result_indices, num_nodes, mapping,nodes = parse_adjlist_LastFM(\n",
    "                    [adjlist[row[mode]] for row in user_artist_batch], [indices[row[mode]] for row in user_artist_batch], samples, offset=offset, mode=mode)\n",
    "            \n",
    "         #   num_edges=len(edges)\n",
    "            \n",
    "        #    print(edges)\n",
    "       #     print(mapping)\n",
    "      #      z=np.random.choice(np.arange(num_edges),int(0.9*num_edges),replace=False)\n",
    "      #      d_edges=[edges[i] for i in z]\n",
    "      #      result_indices=result_indices[z]\n",
    "            d_edges=edges\n",
    "            g = dgl.DGLGraph(multigraph=True)\n",
    "            g.add_nodes(num_nodes)                \n",
    "            if len(d_edges)>0:\n",
    "                sorted_index=sorted(range(len(d_edges)),key=lambda i :d_edges[i])\n",
    "                g.add_edges(*list(zip(*[(d_edges[i][1],d_edges[i][0]) for i in sorted_index]))) \n",
    "                d_result_indices = torch.LongTensor(result_indices[sorted_index]).to(device)\n",
    "                    \n",
    "            else:\n",
    "                d_result_indices=torch.LongTensor(result_indices).to(device)\n",
    "            g=g.to(device)\n",
    "            g_lists[mode].append(g)\n",
    "            result_indices_lists[mode].append(d_result_indices)\n",
    "            idx_batch_mapped_lists[mode].append(np.array([mapping[row[mode]] for row in user_artist_batch]))       \n",
    "            nodes=torch.tensor(nodes).to(device)\n",
    "   #         print(mode)\n",
    "   #         print(nodes)\n",
    "            node_list[mode].append(nodes)\n",
    "    return g_lists, result_indices_lists, idx_batch_mapped_lists,node_list\n",
    "\n",
    "\n",
    "class index_generator:\n",
    "    def __init__(self, batch_size, num_data=None, indices=None, shuffle=True):\n",
    "        if num_data is not None:\n",
    "            self.num_data = num_data\n",
    "            self.indices = np.arange(num_data)\n",
    "        if indices is not None:\n",
    "            self.num_data = len(indices)\n",
    "            self.indices = np.copy(indices)\n",
    "        self.batch_size = batch_size\n",
    "        self.iter_counter = 0\n",
    "        self.shuffle = shuffle\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def next(self):\n",
    "        if self.num_iterations_left() <= 0:\n",
    "            self.reset()\n",
    "        self.iter_counter += 1\n",
    "        return np.copy(self.indices[(self.iter_counter - 1) * self.batch_size:self.iter_counter * self.batch_size])\n",
    "\n",
    "    def num_iterations(self):\n",
    "        return int(np.ceil(self.num_data / self.batch_size))\n",
    "\n",
    "    def num_iterations_left(self):\n",
    "        return self.num_iterations() - self.iter_counter\n",
    "\n",
    "    def reset(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        self.iter_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "welsh-depth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangc/Download/anaconda/envs/workspace/lib/python3.7/site-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n",
      "/home/huangc/Download/anaconda/envs/workspace/lib/python3.7/site-packages/dgl/base.py:45: DGLWarning: Keyword arguments ['multigraph'] are deprecated in v0.5, and can be safely removed in all cases.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Iteration 00000 | Train_Loss 95.3323 | Time1(s) 0.2699 | Time2(s) 0.0359 | Time3(s) 0.0254\n",
      "Epoch 00000 | Iteration 00100 | Train_Loss 0.6620 | Time1(s) 0.3618 | Time2(s) 0.0334 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 00200 | Train_Loss 0.5400 | Time1(s) 0.3619 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 00300 | Train_Loss 0.7270 | Time1(s) 0.3624 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 00400 | Train_Loss 0.7684 | Time1(s) 0.3604 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 00500 | Train_Loss 0.4766 | Time1(s) 0.3592 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 00600 | Train_Loss 0.6218 | Time1(s) 0.3574 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 00700 | Train_Loss 0.4884 | Time1(s) 0.3568 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 00800 | Train_Loss 0.5757 | Time1(s) 0.3556 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 00900 | Train_Loss 0.6102 | Time1(s) 0.3558 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 01000 | Train_Loss 0.4346 | Time1(s) 0.3553 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 01100 | Train_Loss 0.3269 | Time1(s) 0.3544 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 01200 | Train_Loss 0.4688 | Time1(s) 0.3546 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 01300 | Train_Loss 0.7221 | Time1(s) 0.3541 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 01400 | Train_Loss 0.5229 | Time1(s) 0.3543 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Iteration 01500 | Train_Loss 0.6371 | Time1(s) 0.3543 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00000 | Val_Loss 1.5240 | Time(s) 741.7666\n",
      "Validation loss decreased (inf --> 1.523989).  Saving model ...\n",
      "Epoch 00001 | Iteration 00000 | Train_Loss 0.2892 | Time1(s) 0.3539 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 00100 | Train_Loss 0.3814 | Time1(s) 0.3537 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 00200 | Train_Loss 0.3180 | Time1(s) 0.3530 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 00300 | Train_Loss 0.3903 | Time1(s) 0.3526 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 00400 | Train_Loss 0.2865 | Time1(s) 0.3525 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 00500 | Train_Loss 0.3623 | Time1(s) 0.3527 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 00600 | Train_Loss 0.2240 | Time1(s) 0.3529 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 00700 | Train_Loss 0.5995 | Time1(s) 0.3531 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 00800 | Train_Loss 0.5241 | Time1(s) 0.3532 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 00900 | Train_Loss 0.3765 | Time1(s) 0.3533 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 01000 | Train_Loss 0.3424 | Time1(s) 0.3533 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 01100 | Train_Loss 0.2170 | Time1(s) 0.3535 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 01200 | Train_Loss 0.3853 | Time1(s) 0.3536 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 01300 | Train_Loss 0.2162 | Time1(s) 0.3537 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 01400 | Train_Loss 0.3751 | Time1(s) 0.3534 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Iteration 01500 | Train_Loss 0.3800 | Time1(s) 0.3536 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00001 | Val_Loss 1.1189 | Time(s) 741.4298\n",
      "Validation loss decreased (1.523989 --> 1.118938).  Saving model ...\n",
      "Epoch 00002 | Iteration 00000 | Train_Loss 0.7803 | Time1(s) 0.3536 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 00100 | Train_Loss 0.3300 | Time1(s) 0.3538 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 00200 | Train_Loss 0.3962 | Time1(s) 0.3536 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 00300 | Train_Loss 0.6004 | Time1(s) 0.3536 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 00400 | Train_Loss 0.4605 | Time1(s) 0.3535 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 00500 | Train_Loss 0.3485 | Time1(s) 0.3535 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 00600 | Train_Loss 0.6513 | Time1(s) 0.3535 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 00700 | Train_Loss 0.6275 | Time1(s) 0.3537 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 00800 | Train_Loss 0.4209 | Time1(s) 0.3537 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 00900 | Train_Loss 0.2376 | Time1(s) 0.3536 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 01000 | Train_Loss 0.2766 | Time1(s) 0.3537 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 01100 | Train_Loss 0.7404 | Time1(s) 0.3537 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 01200 | Train_Loss 0.4449 | Time1(s) 0.3538 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 01300 | Train_Loss 0.2553 | Time1(s) 0.3537 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 01400 | Train_Loss 0.3913 | Time1(s) 0.3540 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Iteration 01500 | Train_Loss 0.3448 | Time1(s) 0.3539 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00002 | Val_Loss 0.6621 | Time(s) 742.7434\n",
      "Validation loss decreased (1.118938 --> 0.662139).  Saving model ...\n",
      "Epoch 00003 | Iteration 00000 | Train_Loss 0.7047 | Time1(s) 0.3538 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 00100 | Train_Loss 0.4627 | Time1(s) 0.3538 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 00200 | Train_Loss 0.2805 | Time1(s) 0.3538 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 00300 | Train_Loss 0.2033 | Time1(s) 0.3537 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 00400 | Train_Loss 0.3810 | Time1(s) 0.3539 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 00500 | Train_Loss 0.3702 | Time1(s) 0.3538 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 00600 | Train_Loss 0.2836 | Time1(s) 0.3540 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 00700 | Train_Loss 0.5917 | Time1(s) 0.3542 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 00800 | Train_Loss 0.3505 | Time1(s) 0.3543 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 00900 | Train_Loss 0.0803 | Time1(s) 0.3544 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 01000 | Train_Loss 0.5048 | Time1(s) 0.3544 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 01100 | Train_Loss 0.3375 | Time1(s) 0.3542 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 01200 | Train_Loss 0.4138 | Time1(s) 0.3540 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 01300 | Train_Loss 0.5785 | Time1(s) 0.3539 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 01400 | Train_Loss 0.4598 | Time1(s) 0.3540 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Iteration 01500 | Train_Loss 0.4725 | Time1(s) 0.3540 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00003 | Val_Loss 0.6117 | Time(s) 743.8493\n",
      "Validation loss decreased (0.662139 --> 0.611688).  Saving model ...\n",
      "Epoch 00004 | Iteration 00000 | Train_Loss 0.2361 | Time1(s) 0.3541 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 00100 | Train_Loss 0.8893 | Time1(s) 0.3540 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 00200 | Train_Loss 0.3294 | Time1(s) 0.3541 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 00300 | Train_Loss 0.2484 | Time1(s) 0.3540 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 00400 | Train_Loss 0.3580 | Time1(s) 0.3540 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 00500 | Train_Loss 0.3632 | Time1(s) 0.3541 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 00600 | Train_Loss 0.2428 | Time1(s) 0.3540 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 00700 | Train_Loss 0.3318 | Time1(s) 0.3539 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 00800 | Train_Loss 0.1582 | Time1(s) 0.3540 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 00900 | Train_Loss 0.2566 | Time1(s) 0.3541 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 01000 | Train_Loss 0.3971 | Time1(s) 0.3541 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 01100 | Train_Loss 0.3363 | Time1(s) 0.3541 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 01200 | Train_Loss 0.2800 | Time1(s) 0.3541 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 01300 | Train_Loss 0.4519 | Time1(s) 0.3541 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 01400 | Train_Loss 0.4928 | Time1(s) 0.3540 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Iteration 01500 | Train_Loss 0.2643 | Time1(s) 0.3542 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00004 | Val_Loss 0.7811 | Time(s) 742.8049\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch 00005 | Iteration 00000 | Train_Loss 0.2358 | Time1(s) 0.3541 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 00100 | Train_Loss 0.0705 | Time1(s) 0.3542 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 00200 | Train_Loss 0.1766 | Time1(s) 0.3542 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 00300 | Train_Loss 0.5857 | Time1(s) 0.3542 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 00400 | Train_Loss 0.2565 | Time1(s) 0.3542 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 00500 | Train_Loss 0.3926 | Time1(s) 0.3542 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 00600 | Train_Loss 0.3738 | Time1(s) 0.3541 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 00700 | Train_Loss 0.3617 | Time1(s) 0.3543 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 00800 | Train_Loss 0.3270 | Time1(s) 0.3542 | Time2(s) 0.0336 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 00900 | Train_Loss 0.2480 | Time1(s) 0.3542 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 01000 | Train_Loss 0.4315 | Time1(s) 0.3542 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 01100 | Train_Loss 0.1624 | Time1(s) 0.3542 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 01200 | Train_Loss 0.1560 | Time1(s) 0.3543 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 01300 | Train_Loss 0.3346 | Time1(s) 0.3543 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 01400 | Train_Loss 0.3174 | Time1(s) 0.3542 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00005 | Iteration 01500 | Train_Loss 0.5203 | Time1(s) 0.3542 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00005 | Val_Loss 0.9227 | Time(s) 743.9572\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Epoch 00006 | Iteration 00000 | Train_Loss 0.7834 | Time1(s) 0.3543 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 00100 | Train_Loss 0.4633 | Time1(s) 0.3543 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 00200 | Train_Loss 0.4004 | Time1(s) 0.3543 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 00300 | Train_Loss 0.4847 | Time1(s) 0.3544 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 00400 | Train_Loss 0.2363 | Time1(s) 0.3543 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 00500 | Train_Loss 0.3105 | Time1(s) 0.3542 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 00600 | Train_Loss 0.3425 | Time1(s) 0.3541 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 00700 | Train_Loss 0.1744 | Time1(s) 0.3541 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 00800 | Train_Loss 0.2979 | Time1(s) 0.3542 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 00900 | Train_Loss 0.4828 | Time1(s) 0.3541 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 01000 | Train_Loss 0.4457 | Time1(s) 0.3541 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 01100 | Train_Loss 0.3017 | Time1(s) 0.3541 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 01200 | Train_Loss 0.2423 | Time1(s) 0.3541 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 01300 | Train_Loss 0.3629 | Time1(s) 0.3540 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 01400 | Train_Loss 0.5583 | Time1(s) 0.3540 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Iteration 01500 | Train_Loss 0.2192 | Time1(s) 0.3541 | Time2(s) 0.0335 | Time3(s) 0.0231\n",
      "Epoch 00006 | Val_Loss 0.6995 | Time(s) 740.6813\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping!\n",
      "Link Prediction Test\n",
      "AUC = 0.9468331099040043\n",
      "AP = 0.9490429262913522\n",
      "----------------------------------------------------------------\n",
      "Link Prediction Tests Summary\n",
      "AUC_mean = 0.9468331099040043, AUC_std = 0.0\n",
      "AP_mean = 0.9490429262913522, AP_std = 0.0\n"
     ]
    }
   ],
   "source": [
    "num_ntype = 3\n",
    "dropout_rate = 0.5\n",
    "lr = 0.005\n",
    "weight_decay = 0.001\n",
    "#etypes_lists = [[[0, 1],[0, 2, 3, 1]],\n",
    " #               [[1, 0],[2, 3]]\n",
    "etypes_lists = [[[0, 1],[0, 2, 3, 1],[4,None,5]],\n",
    "                [[1, 0],[2, 3],[2, None, 3]]]\n",
    "use_masks = [[True, True],\n",
    "             [True, False]]\n",
    "no_masks = [[False] * 3, [False] * 3]\n",
    "#use_masks = [[True, True, False],\n",
    "#             [True, False, True]]\n",
    "#no_masks = [[False] * 3, [False] * 3]\n",
    "num_mir=1296\n",
    "num_disease=11783\n",
    "num_gene=10116\n",
    "expected_metapaths = [\n",
    "   [(0, 1, 0), (0, 1, 2, 1, 0), (0,2,2,0)],\n",
    "    [(1, 0, 1), (1, 2, 1), (1, 2, 2, 1)]\n",
    "]\n",
    "f='0727_5.txt'\n",
    "\n",
    "def run_model_OURS(feats_type, hidden_dim, num_heads, attn_vec_dim,\n",
    "                     num_epochs, patience, batch_size, neighbor_samples, repeat,save_postfix):\n",
    "    adjlists_ua, edge_metapath_indices_list_ua, _, type_mask, train_val_test_pos_user_artist, train_val_test_neg_user_artist = load_LastFM_data()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    features_list = []\n",
    "    in_dims = []\n",
    "    if feats_type == 0:\n",
    "        for i in range(num_ntype):\n",
    "            dim = (type_mask == i).sum()\n",
    "            in_dims.append(dim)\n",
    "            indices = np.vstack((np.arange(dim), np.arange(dim)))\n",
    "            indices = torch.LongTensor(indices)\n",
    "            values = torch.FloatTensor(np.ones(dim))\n",
    "            features_list.append(torch.sparse.FloatTensor(indices, values, torch.Size([dim, dim])).to(device))\n",
    "    elif feats_type == 1:\n",
    "        for i in range(num_ntype):\n",
    "            dim = 10\n",
    "            num_nodes = (type_mask == i).sum()\n",
    "            in_dims.append(dim)\n",
    "            features_list.append(torch.zeros((num_nodes, 10)).to(device))\n",
    "    train_pos_user_artist = train_val_test_pos_user_artist['train_pos_mir_disease']\n",
    "    val_pos_user_artist = train_val_test_pos_user_artist['val_pos_mir_disease']\n",
    "    test_pos_user_artist = train_val_test_pos_user_artist['test_pos_mir_disease']\n",
    "    train_neg_user_artist = train_val_test_neg_user_artist['train_neg_mir_disease']\n",
    "    val_neg_user_artist = train_val_test_neg_user_artist['val_neg_mir_disease']\n",
    "    test_neg_user_artist = train_val_test_neg_user_artist['test_neg_mir_disease']\n",
    "    y_true_test = np.array([1] * len(test_pos_user_artist) + [0] * len(test_neg_user_artist))\n",
    "\n",
    "    auc_list = []\n",
    "    ap_list = []\n",
    "\n",
    "    with open(f,\"a\") as file:\n",
    "        file.write('单层dropout0.6+torch.mm(features,self.weight)'+\"\\n\")\n",
    "    for _ in range(repeat):\n",
    "        net = MEAHNE_lp(\n",
    "            [3, 3], 6, etypes_lists, in_dims, hidden_dim, hidden_dim, num_heads, attn_vec_dim, dropout_rate)\n",
    "        net.to(device)\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # training loop\n",
    "        net.train()\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True, save_path='checkpoint/checkpoint_{}.pt'.format(save_postfix))\n",
    "        dur1 = []\n",
    "        dur2 = []\n",
    "        dur3 = []\n",
    "        train_pos_idx_generator = index_generator(batch_size=batch_size, num_data=len(train_pos_user_artist))\n",
    "        val_idx_generator = index_generator(batch_size=batch_size, num_data=len(val_pos_user_artist), shuffle=False)\n",
    "        for epoch in range(num_epochs):\n",
    "            t_start = time.time()\n",
    "            # training\n",
    "            net.train()\n",
    "            for iteration in range(train_pos_idx_generator.num_iterations()):\n",
    "                # forward\n",
    "                t0 = time.time()\n",
    "\n",
    "                train_pos_idx_batch = train_pos_idx_generator.next()\n",
    "                train_pos_idx_batch.sort()\n",
    "               \n",
    "                train_pos_user_artist_batch = train_pos_user_artist[train_pos_idx_batch].tolist()\n",
    "                train_neg_idx_batch = np.random.choice(len(train_neg_user_artist), len(train_pos_idx_batch))\n",
    "                train_neg_idx_batch.sort()\n",
    "                train_neg_user_artist_batch = train_neg_user_artist[train_neg_idx_batch].tolist()\n",
    "                \n",
    "                #shuffle\n",
    "                num_pos = train_pos_idx_batch.shape[0]\n",
    "                train_batch = np.concatenate([train_pos_user_artist_batch, train_neg_user_artist_batch], axis=0)\n",
    "                y_label = np.zeros((train_batch.shape[0], 1), dtype=int)\n",
    "                y_label[:num_pos] = 1\n",
    "                train_data = np.concatenate([train_batch, y_label], axis=1)\n",
    "                np.random.shuffle(train_data)\n",
    "                train_batch = train_data[:, :-1]\n",
    "                y_label = train_data[:, -1]\n",
    "\n",
    "                train_g_lists, train_indices_lists, train_idx_batch_mapped_lists ,node_lists= parse_minibatch_LastFM(\n",
    "                   adjlists_ua, edge_metapath_indices_list_ua, train_batch, device, neighbor_samples, no_masks, num_mir)\n",
    "                t1 = time.time()\n",
    "                dur1.append(t1 - t0)\n",
    "\n",
    "                [embedding_user, embedding_artist], _ = net(\n",
    "                    (train_g_lists, features_list, type_mask, train_indices_lists, train_idx_batch_mapped_lists,node_lists))\n",
    "                \n",
    "                embedding_user = embedding_user.view(-1, 1, embedding_user.shape[1])\n",
    "                embedding_artist = embedding_artist.view(-1, embedding_artist.shape[1], 1)\n",
    "                \n",
    "                out = torch.bmm(embedding_user, embedding_artist)\n",
    "                class_op = torch.LongTensor([1 if l == 1 else -1 for l in y_label]).view(-1, 1, 1).to(device)\n",
    "                \n",
    "                train_loss = -torch.mean(F.logsigmoid(out * class_op))#-torch.mean(F.logsigmoid(net.get_loss()*0.5))\n",
    "                \n",
    "                t2 = time.time()\n",
    "                dur2.append(t2 - t1)\n",
    "\n",
    "                # autograd\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                t3 = time.time()\n",
    "                dur3.append(t3 - t2)\n",
    "\n",
    "                # print training info\n",
    "                if iteration % 100 == 0:\n",
    "                    print(\n",
    "                        'Epoch {:05d} | Iteration {:05d} | Train_Loss {:.4f} | Time1(s) {:.4f} | Time2(s) {:.4f} | Time3(s) {:.4f}'.format(\n",
    "                            epoch, iteration, train_loss.item(), np.mean(dur1), np.mean(dur2), np.mean(dur3)))\n",
    "                    with open(f,\"a\") as file:\n",
    "                        file.write('epoch:'+str(epoch)+'iteration:'+str(iteration)+'train_loss:'+str(train_loss.item())+\"time1:\"+ str(np.mean(dur1))+'time2'\n",
    "                               +str(np.mean(dur2))+'time3'+str(np.mean(dur3))+\"\\n\")    \n",
    "            # validation\n",
    "            net.eval()\n",
    "            val_loss = []\n",
    "            with torch.no_grad():\n",
    "                for iteration in range(val_idx_generator.num_iterations()):\n",
    "                    # forward\n",
    "                    val_idx_batch = val_idx_generator.next()\n",
    "                    val_pos_user_artist_batch = val_pos_user_artist[val_idx_batch].tolist()\n",
    "                    val_neg_user_artist_batch = val_neg_user_artist[val_idx_batch].tolist()\n",
    "                    val_pos_g_lists, val_pos_indices_lists, val_pos_idx_batch_mapped_lists,val_pos_node_lists = parse_minibatch_LastFM(\n",
    "                        adjlists_ua, edge_metapath_indices_list_ua, val_pos_user_artist_batch, device, neighbor_samples, no_masks, num_mir)\n",
    "                    val_neg_g_lists, val_neg_indices_lists, val_neg_idx_batch_mapped_lists,val_neg_node_lists= parse_minibatch_LastFM(\n",
    "                        adjlists_ua, edge_metapath_indices_list_ua, val_neg_user_artist_batch, device, neighbor_samples, no_masks, num_mir)\n",
    "\n",
    "                    [pos_embedding_user, pos_embedding_artist], _ = net(\n",
    "                        (val_pos_g_lists, features_list, type_mask, val_pos_indices_lists, val_pos_idx_batch_mapped_lists,val_pos_node_lists))\n",
    "                    [neg_embedding_user, neg_embedding_artist], _ = net(\n",
    "                        (val_neg_g_lists, features_list, type_mask, val_neg_indices_lists, val_neg_idx_batch_mapped_lists,val_neg_node_lists))\n",
    "                    pos_embedding_user = pos_embedding_user.view(-1, 1, pos_embedding_user.shape[1])\n",
    "                    pos_embedding_artist = pos_embedding_artist.view(-1, pos_embedding_artist.shape[1], 1)\n",
    "                    neg_embedding_user = neg_embedding_user.view(-1, 1, neg_embedding_user.shape[1])\n",
    "                    neg_embedding_artist = neg_embedding_artist.view(-1, neg_embedding_artist.shape[1], 1)\n",
    "\n",
    "                    pos_out = torch.bmm(pos_embedding_user, pos_embedding_artist)\n",
    "                    neg_out = -torch.bmm(neg_embedding_user, neg_embedding_artist)\n",
    "                    val_loss.append(-torch.mean(F.logsigmoid(pos_out) + F.logsigmoid(neg_out)))#-torch.mean(F.logsigmoid(net.get_loss()*0.5)))\n",
    "                val_loss = torch.mean(torch.tensor(val_loss))\n",
    "            t_end = time.time()\n",
    "            # print validation info\n",
    "            print('Epoch {:05d} | Val_Loss {:.4f} | Time(s) {:.4f}'.format(\n",
    "                epoch, val_loss.item(), t_end - t_start))\n",
    "            with open(f,\"a\") as file:\n",
    "                file.write('epoch:'+str(epoch)+'val_loss:'+str(val_loss.item())+\"time:\"+str(t_end-t_start)+\"\\n\")\n",
    "            # early stopping\n",
    "            early_stopping(val_loss, net)\n",
    "            if early_stopping.early_stop:\n",
    "                print('Early stopping!')\n",
    "                with open(f,\"a\") as file:\n",
    "                    file.write('epoch:'+str(epoch)+'early stopping!'+\"\\n\")\n",
    "                break\n",
    "            \n",
    "        test_idx_generator = index_generator(batch_size=batch_size, num_data=len(test_pos_user_artist), shuffle=False)\n",
    "        net.load_state_dict(torch.load('checkpoint/checkpoint_{}.pt'.format(save_postfix)))\n",
    "        net.eval()\n",
    "        pos_proba_list = []\n",
    "        neg_proba_list = []\n",
    "        with torch.no_grad():\n",
    "            for iteration in range(test_idx_generator.num_iterations()):\n",
    "                # forward\n",
    "                test_idx_batch = test_idx_generator.next()\n",
    "                test_pos_user_artist_batch = test_pos_user_artist[test_idx_batch].tolist()\n",
    "                test_neg_user_artist_batch = test_neg_user_artist[test_idx_batch].tolist()\n",
    "                test_pos_g_lists, test_pos_indices_lists, test_pos_idx_batch_mapped_lists,test_pos_node_lists = parse_minibatch_LastFM(\n",
    "                    adjlists_ua, edge_metapath_indices_list_ua, test_pos_user_artist_batch, device, neighbor_samples, no_masks, num_mir)\n",
    "                test_neg_g_lists, test_neg_indices_lists, test_neg_idx_batch_mapped_lists,test_neg_node_lists = parse_minibatch_LastFM(\n",
    "                    adjlists_ua, edge_metapath_indices_list_ua, test_neg_user_artist_batch, device, neighbor_samples, no_masks, num_mir)\n",
    "\n",
    "                [pos_embedding_user, pos_embedding_artist], _ = net(\n",
    "                    (test_pos_g_lists, features_list, type_mask, test_pos_indices_lists, test_pos_idx_batch_mapped_lists,test_pos_node_lists))\n",
    "                \n",
    "                [neg_embedding_user, neg_embedding_artist], _ = net(\n",
    "                    (test_neg_g_lists, features_list, type_mask, test_neg_indices_lists, test_neg_idx_batch_mapped_lists,test_neg_node_lists))\n",
    "                pos_embedding_user = pos_embedding_user.view(-1, 1, pos_embedding_user.shape[1])\n",
    "                pos_embedding_artist = pos_embedding_artist.view(-1, pos_embedding_artist.shape[1], 1)\n",
    "                neg_embedding_user = neg_embedding_user.view(-1, 1, neg_embedding_user.shape[1])\n",
    "                neg_embedding_artist = neg_embedding_artist.view(-1, neg_embedding_artist.shape[1], 1)\n",
    "\n",
    "                pos_out = torch.bmm(pos_embedding_user, pos_embedding_artist).flatten()\n",
    "                neg_out = torch.bmm(neg_embedding_user, neg_embedding_artist).flatten()\n",
    "                pos_proba_list.append(torch.sigmoid(pos_out))\n",
    "                neg_proba_list.append(torch.sigmoid(neg_out))\n",
    "            y_proba_test = torch.cat(pos_proba_list + neg_proba_list)\n",
    "            y_proba_test = y_proba_test.cpu().numpy()\n",
    "        np.savez('OURS_prediction_result.npz', y_true=y_true_test,\n",
    "                y_pred=y_proba_test)    \n",
    "        auc = roc_auc_score(y_true_test, y_proba_test)\n",
    "        ap = average_precision_score(y_true_test, y_proba_test)\n",
    "        print('Link Prediction Test')\n",
    "        print('AUC = {}'.format(auc))\n",
    "        print('AP = {}'.format(ap))\n",
    "        auc_list.append(auc)\n",
    "        ap_list.append(ap)\n",
    "        with open(f,\"a\") as file:\n",
    "                file.write('Link Prediction Test:'+\"\\n\"+'AUC = {}:'+str(auc)+\"|\"+\"AP = {}:\"+str(ap)+\"\\n\")\n",
    "    print('----------------------------------------------------------------')\n",
    "    print('Link Prediction Tests Summary')\n",
    "    print('AUC_mean = {}, AUC_std = {}'.format(np.mean(auc_list), np.std(auc_list)))\n",
    "    print('AP_mean = {}, AP_std = {}'.format(np.mean(ap_list), np.std(ap_list)))\n",
    "    with open(f,\"a\") as file:\n",
    "        file.write('Link Prediction Tests Summary:'+\"\\n\"+\"AUC_mean = {}\"+str(np.mean(auc_list))+'AUC_std = {}'+str(np.std(auc_list))+\n",
    "                           \"\\n\"+'AP_mean = {} '+str(np.mean(ap_list))+'AP_std = {}'+str(np.std(ap_list))+\"\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  #  ap = argparse.ArgumentParser(description='MRGNN testing for the recommendation dataset')\n",
    "   # ap.add_argument('--feats-type', type=int, default=0,\n",
    "   #                 help='Type of the node features used. ' +\n",
    "  #                       '0 - all id vectors; ' +\n",
    " #                        '1 - all zero vector. Default is 0.')\n",
    "  #  ap.add_argument('--hidden-dim', type=int, default=64, help='Dimension of the node hidden state. Default is 64.')\n",
    " #   ap.add_argument('--num-heads', type=int, default=8, help='Number of the attention heads. Default is 8.')\n",
    "  #  ap.add_argument('--attn-vec-dim', type=int, default=128, help='Dimension of the attention vector. Default is 128.')\n",
    "  #  ap.add_argument('--rnn-type', default='RotatE0', help='Type of the aggregator. Default is RotatE0.')\n",
    " #   ap.add_argument('--epoch', type=int, default=100, help='Number of epochs. Default is 100.')\n",
    "  #  ap.add_argument('--patience', type=int, default=5, help='Patience. Default is 5.')\n",
    "  #  ap.add_argument('--batch-size', type=int, default=8, help='Batch size. Default is 8.')\n",
    "   # ap.add_argument('--samples', type=int, default=100, help='Number of neighbors sampled. Default is 100.')\n",
    "    #ap.add_argument('--repeat', type=int, default=1, help='Repeat the training and testing for N times. Default is 1.')\n",
    "   # ap.add_argument('--save-postfix', default='LastFM', help='Postfix for the saved model and result. Default is LastFM.')\n",
    "\n",
    "  #  args = ap.parse_args()\n",
    "    run_model_OURS(0, 64, 1,128, 100,3,8, 100, 1, 'ours')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "joint-subcommittee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 2142], [1, 2142], [2, 2142], [3, 2142], [4, 2142], [5, 2142], [6, 2142], [7, 2142]]\n",
      "[[8, 2142], [9, 2142], [10, 2142], [11, 2142], [12, 2142], [13, 2142], [14, 2142], [15, 2142]]\n",
      "[[16, 2142], [17, 2142], [18, 2142], [19, 2142], [20, 2142], [21, 2142], [22, 2142], [23, 2142]]\n",
      "[[24, 2142], [25, 2142], [26, 2142], [27, 2142], [28, 2142], [29, 2142], [30, 2142], [31, 2142]]\n",
      "[[32, 2142], [33, 2142], [34, 2142], [35, 2142], [36, 2142], [37, 2142], [38, 2142], [39, 2142]]\n",
      "[[40, 2142], [41, 2142], [42, 2142], [43, 2142], [44, 2142], [45, 2142], [46, 2142], [47, 2142]]\n",
      "[[48, 2142], [49, 2142], [50, 2142], [51, 2142], [52, 2142], [53, 2142], [54, 2142], [55, 2142]]\n",
      "[[56, 2142], [57, 2142], [58, 2142], [59, 2142], [60, 2142], [61, 2142], [62, 2142], [63, 2142]]\n",
      "[[64, 2142], [65, 2142], [66, 2142], [67, 2142], [68, 2142], [69, 2142], [70, 2142], [71, 2142]]\n",
      "[[72, 2142], [73, 2142], [74, 2142], [75, 2142], [76, 2142], [77, 2142], [78, 2142], [79, 2142]]\n",
      "[[80, 2142], [81, 2142], [82, 2142], [83, 2142], [84, 2142], [85, 2142], [86, 2142], [87, 2142]]\n",
      "[[88, 2142], [89, 2142], [90, 2142], [91, 2142], [92, 2142], [93, 2142], [94, 2142], [95, 2142]]\n",
      "[[96, 2142], [97, 2142], [98, 2142], [99, 2142], [100, 2142], [101, 2142], [102, 2142], [103, 2142]]\n",
      "[[104, 2142], [105, 2142], [106, 2142], [107, 2142], [108, 2142], [109, 2142], [110, 2142], [111, 2142]]\n",
      "[[112, 2142], [113, 2142], [114, 2142], [115, 2142], [116, 2142], [117, 2142], [118, 2142], [119, 2142]]\n",
      "[[120, 2142], [121, 2142], [122, 2142], [123, 2142], [124, 2142], [125, 2142], [126, 2142], [127, 2142]]\n",
      "[[128, 2142], [129, 2142], [130, 2142], [131, 2142], [132, 2142], [133, 2142], [134, 2142], [135, 2142]]\n",
      "[[136, 2142], [137, 2142], [138, 2142], [139, 2142], [140, 2142], [141, 2142], [142, 2142], [143, 2142]]\n",
      "[[144, 2142], [145, 2142], [146, 2142], [147, 2142], [148, 2142], [149, 2142], [150, 2142], [151, 2142]]\n",
      "[[152, 2142], [153, 2142], [154, 2142], [155, 2142], [156, 2142], [157, 2142], [158, 2142], [159, 2142]]\n",
      "[[160, 2142], [161, 2142], [162, 2142], [163, 2142], [164, 2142], [165, 2142], [166, 2142], [167, 2142]]\n",
      "[[168, 2142], [169, 2142], [170, 2142], [171, 2142], [172, 2142], [173, 2142], [174, 2142], [175, 2142]]\n",
      "[[176, 2142], [177, 2142], [178, 2142], [179, 2142], [180, 2142], [181, 2142], [182, 2142], [183, 2142]]\n",
      "[[184, 2142], [185, 2142], [186, 2142], [187, 2142], [188, 2142], [189, 2142], [190, 2142], [191, 2142]]\n",
      "[[192, 2142], [193, 2142], [194, 2142], [195, 2142], [196, 2142], [197, 2142], [198, 2142], [199, 2142]]\n",
      "[[200, 2142], [201, 2142], [202, 2142], [203, 2142], [204, 2142], [205, 2142], [206, 2142], [207, 2142]]\n",
      "[[208, 2142], [209, 2142], [210, 2142], [211, 2142], [212, 2142], [213, 2142], [214, 2142], [215, 2142]]\n",
      "[[216, 2142], [217, 2142], [218, 2142], [219, 2142], [220, 2142], [221, 2142], [222, 2142], [223, 2142]]\n",
      "[[224, 2142], [225, 2142], [226, 2142], [227, 2142], [228, 2142], [229, 2142], [230, 2142], [231, 2142]]\n",
      "[[232, 2142], [233, 2142], [234, 2142], [235, 2142], [236, 2142], [237, 2142], [238, 2142], [239, 2142]]\n",
      "[[240, 2142], [241, 2142], [242, 2142], [243, 2142], [244, 2142], [245, 2142], [246, 2142], [247, 2142]]\n",
      "[[248, 2142], [249, 2142], [250, 2142], [251, 2142], [252, 2142], [253, 2142], [254, 2142], [255, 2142]]\n",
      "[[256, 2142], [257, 2142], [258, 2142], [259, 2142], [260, 2142], [261, 2142], [262, 2142], [263, 2142]]\n",
      "[[264, 2142], [265, 2142], [266, 2142], [267, 2142], [268, 2142], [269, 2142], [270, 2142], [271, 2142]]\n",
      "[[272, 2142], [273, 2142], [274, 2142], [275, 2142], [276, 2142], [277, 2142], [278, 2142], [279, 2142]]\n",
      "[[280, 2142], [281, 2142], [282, 2142], [283, 2142], [284, 2142], [285, 2142], [286, 2142], [287, 2142]]\n",
      "[[288, 2142], [289, 2142], [290, 2142], [291, 2142], [292, 2142], [293, 2142], [294, 2142], [295, 2142]]\n",
      "[[296, 2142], [297, 2142], [298, 2142], [299, 2142], [300, 2142], [301, 2142], [302, 2142], [303, 2142]]\n",
      "[[304, 2142], [305, 2142], [306, 2142], [307, 2142], [308, 2142], [309, 2142], [310, 2142], [311, 2142]]\n",
      "[[312, 2142], [313, 2142], [314, 2142], [315, 2142], [316, 2142], [317, 2142], [318, 2142], [319, 2142]]\n",
      "[[320, 2142], [321, 2142], [322, 2142], [323, 2142], [324, 2142], [325, 2142], [326, 2142], [327, 2142]]\n",
      "[[328, 2142], [329, 2142], [330, 2142], [331, 2142], [332, 2142], [333, 2142], [334, 2142], [335, 2142]]\n",
      "[[336, 2142], [337, 2142], [338, 2142], [339, 2142], [340, 2142], [341, 2142], [342, 2142], [343, 2142]]\n",
      "[[344, 2142], [345, 2142], [346, 2142], [347, 2142], [348, 2142], [349, 2142], [350, 2142], [351, 2142]]\n",
      "[[352, 2142], [353, 2142], [354, 2142], [355, 2142], [356, 2142], [357, 2142], [358, 2142], [359, 2142]]\n",
      "[[360, 2142], [361, 2142], [362, 2142], [363, 2142], [364, 2142], [365, 2142], [366, 2142], [367, 2142]]\n",
      "[[368, 2142], [369, 2142], [370, 2142], [371, 2142], [372, 2142], [373, 2142], [374, 2142], [375, 2142]]\n",
      "[[376, 2142], [377, 2142], [378, 2142], [379, 2142], [380, 2142], [381, 2142], [382, 2142], [383, 2142]]\n",
      "[[384, 2142], [385, 2142], [386, 2142], [387, 2142], [388, 2142], [389, 2142], [390, 2142], [391, 2142]]\n",
      "[[392, 2142], [393, 2142], [394, 2142], [395, 2142], [396, 2142], [397, 2142], [398, 2142], [399, 2142]]\n",
      "[[400, 2142], [401, 2142], [402, 2142], [403, 2142], [404, 2142], [405, 2142], [406, 2142], [407, 2142]]\n",
      "[[408, 2142], [409, 2142], [410, 2142], [411, 2142], [412, 2142], [413, 2142], [414, 2142], [415, 2142]]\n",
      "[[416, 2142], [417, 2142], [418, 2142], [419, 2142], [420, 2142], [421, 2142], [422, 2142], [423, 2142]]\n",
      "[[424, 2142], [425, 2142], [426, 2142], [427, 2142], [428, 2142], [429, 2142], [430, 2142], [431, 2142]]\n",
      "[[432, 2142], [433, 2142], [434, 2142], [435, 2142], [436, 2142], [437, 2142], [438, 2142], [439, 2142]]\n",
      "[[440, 2142], [441, 2142], [442, 2142], [443, 2142], [444, 2142], [445, 2142], [446, 2142], [447, 2142]]\n",
      "[[448, 2142], [449, 2142], [450, 2142], [451, 2142], [452, 2142], [453, 2142], [454, 2142], [455, 2142]]\n",
      "[[456, 2142], [457, 2142], [458, 2142], [459, 2142], [460, 2142], [461, 2142], [462, 2142], [463, 2142]]\n",
      "[[464, 2142], [465, 2142], [466, 2142], [467, 2142], [468, 2142], [469, 2142], [470, 2142], [471, 2142]]\n",
      "[[472, 2142], [473, 2142], [474, 2142], [475, 2142], [476, 2142], [477, 2142], [478, 2142], [479, 2142]]\n",
      "[[480, 2142], [481, 2142], [482, 2142], [483, 2142], [484, 2142], [485, 2142], [486, 2142], [487, 2142]]\n",
      "[[488, 2142], [489, 2142], [490, 2142], [491, 2142], [492, 2142], [493, 2142], [494, 2142], [495, 2142]]\n",
      "[[496, 2142], [497, 2142], [498, 2142], [499, 2142], [500, 2142], [501, 2142], [502, 2142], [503, 2142]]\n",
      "[[504, 2142], [505, 2142], [506, 2142], [507, 2142], [508, 2142], [509, 2142], [510, 2142], [511, 2142]]\n",
      "[[512, 2142], [513, 2142], [514, 2142], [515, 2142], [516, 2142], [517, 2142], [518, 2142], [519, 2142]]\n",
      "[[520, 2142], [521, 2142], [522, 2142], [523, 2142], [524, 2142], [525, 2142], [526, 2142], [527, 2142]]\n",
      "[[528, 2142], [529, 2142], [530, 2142], [531, 2142], [532, 2142], [533, 2142], [534, 2142], [535, 2142]]\n",
      "[[536, 2142], [537, 2142], [538, 2142], [539, 2142], [540, 2142], [541, 2142], [542, 2142], [543, 2142]]\n",
      "[[544, 2142], [545, 2142], [546, 2142], [547, 2142], [548, 2142], [549, 2142], [550, 2142], [551, 2142]]\n",
      "[[552, 2142], [553, 2142], [554, 2142], [555, 2142], [556, 2142], [557, 2142], [558, 2142], [559, 2142]]\n",
      "[[560, 2142], [561, 2142], [562, 2142], [563, 2142], [564, 2142], [565, 2142], [566, 2142], [567, 2142]]\n",
      "[[568, 2142], [569, 2142], [570, 2142], [571, 2142], [572, 2142], [573, 2142], [574, 2142], [575, 2142]]\n",
      "[[576, 2142], [577, 2142], [578, 2142], [579, 2142], [580, 2142], [581, 2142], [582, 2142], [583, 2142]]\n",
      "[[584, 2142], [585, 2142], [586, 2142], [587, 2142], [588, 2142], [589, 2142], [590, 2142], [591, 2142]]\n",
      "[[592, 2142], [593, 2142], [594, 2142], [595, 2142], [596, 2142], [597, 2142], [598, 2142], [599, 2142]]\n",
      "[[600, 2142], [601, 2142], [602, 2142], [603, 2142], [604, 2142], [605, 2142], [606, 2142], [607, 2142]]\n",
      "[[608, 2142], [609, 2142], [610, 2142], [611, 2142], [612, 2142], [613, 2142], [614, 2142], [615, 2142]]\n",
      "[[616, 2142], [617, 2142], [618, 2142], [619, 2142], [620, 2142], [621, 2142], [622, 2142], [623, 2142]]\n",
      "[[624, 2142], [625, 2142], [626, 2142], [627, 2142], [628, 2142], [629, 2142], [630, 2142], [631, 2142]]\n",
      "[[632, 2142], [633, 2142], [634, 2142], [635, 2142], [636, 2142], [637, 2142], [638, 2142], [639, 2142]]\n",
      "[[640, 2142], [641, 2142], [642, 2142], [643, 2142], [644, 2142], [645, 2142], [646, 2142], [647, 2142]]\n",
      "[[648, 2142], [649, 2142], [650, 2142], [651, 2142], [652, 2142], [653, 2142], [654, 2142], [655, 2142]]\n",
      "[[656, 2142], [657, 2142], [658, 2142], [659, 2142], [660, 2142], [661, 2142], [662, 2142], [663, 2142]]\n",
      "[[664, 2142], [665, 2142], [666, 2142], [667, 2142], [668, 2142], [669, 2142], [670, 2142], [671, 2142]]\n",
      "[[672, 2142], [673, 2142], [674, 2142], [675, 2142], [676, 2142], [677, 2142], [678, 2142], [679, 2142]]\n",
      "[[680, 2142], [681, 2142], [682, 2142], [683, 2142], [684, 2142], [685, 2142], [686, 2142], [687, 2142]]\n",
      "[[688, 2142], [689, 2142], [690, 2142], [691, 2142], [692, 2142], [693, 2142], [694, 2142], [695, 2142]]\n",
      "[[696, 2142], [697, 2142], [698, 2142], [699, 2142], [700, 2142], [701, 2142], [702, 2142], [703, 2142]]\n",
      "[[704, 2142], [705, 2142], [706, 2142], [707, 2142], [708, 2142], [709, 2142], [710, 2142], [711, 2142]]\n",
      "[[712, 2142], [713, 2142], [714, 2142], [715, 2142], [716, 2142], [717, 2142], [718, 2142], [719, 2142]]\n",
      "[[720, 2142], [721, 2142], [722, 2142], [723, 2142], [724, 2142], [725, 2142], [726, 2142], [727, 2142]]\n",
      "[[728, 2142], [729, 2142], [730, 2142], [731, 2142], [732, 2142], [733, 2142], [734, 2142], [735, 2142]]\n",
      "[[736, 2142], [737, 2142], [738, 2142], [739, 2142], [740, 2142], [741, 2142], [742, 2142], [743, 2142]]\n",
      "[[744, 2142], [745, 2142], [746, 2142], [747, 2142], [748, 2142], [749, 2142], [750, 2142], [751, 2142]]\n",
      "[[752, 2142], [753, 2142], [754, 2142], [755, 2142], [756, 2142], [757, 2142], [758, 2142], [759, 2142]]\n",
      "[[760, 2142], [761, 2142], [762, 2142], [763, 2142], [764, 2142], [765, 2142], [766, 2142], [767, 2142]]\n",
      "[[768, 2142], [769, 2142], [770, 2142], [771, 2142], [772, 2142], [773, 2142], [774, 2142], [775, 2142]]\n",
      "[[776, 2142], [777, 2142], [778, 2142], [779, 2142], [780, 2142], [781, 2142], [782, 2142], [783, 2142]]\n",
      "[[784, 2142], [785, 2142], [786, 2142], [787, 2142], [788, 2142], [789, 2142], [790, 2142], [791, 2142]]\n",
      "[[792, 2142], [793, 2142], [794, 2142], [795, 2142], [796, 2142], [797, 2142], [798, 2142], [799, 2142]]\n",
      "[[800, 2142], [801, 2142], [802, 2142], [803, 2142], [804, 2142], [805, 2142], [806, 2142], [807, 2142]]\n",
      "[[808, 2142], [809, 2142], [810, 2142], [811, 2142], [812, 2142], [813, 2142], [814, 2142], [815, 2142]]\n",
      "[[816, 2142], [817, 2142], [818, 2142], [819, 2142], [820, 2142], [821, 2142], [822, 2142], [823, 2142]]\n",
      "[[824, 2142], [825, 2142], [826, 2142], [827, 2142], [828, 2142], [829, 2142], [830, 2142], [831, 2142]]\n",
      "[[832, 2142], [833, 2142], [834, 2142], [835, 2142], [836, 2142], [837, 2142], [838, 2142], [839, 2142]]\n",
      "[[840, 2142], [841, 2142], [842, 2142], [843, 2142], [844, 2142], [845, 2142], [846, 2142], [847, 2142]]\n",
      "[[848, 2142], [849, 2142], [850, 2142], [851, 2142], [852, 2142], [853, 2142], [854, 2142], [855, 2142]]\n",
      "[[856, 2142], [857, 2142], [858, 2142], [859, 2142], [860, 2142], [861, 2142], [862, 2142], [863, 2142]]\n",
      "[[864, 2142], [865, 2142], [866, 2142], [867, 2142], [868, 2142], [869, 2142], [870, 2142], [871, 2142]]\n",
      "[[872, 2142], [873, 2142], [874, 2142], [875, 2142], [876, 2142], [877, 2142], [878, 2142], [879, 2142]]\n",
      "[[880, 2142], [881, 2142], [882, 2142], [883, 2142], [884, 2142], [885, 2142], [886, 2142], [887, 2142]]\n",
      "[[888, 2142], [889, 2142], [890, 2142], [891, 2142], [892, 2142], [893, 2142], [894, 2142], [895, 2142]]\n",
      "[[896, 2142], [897, 2142], [898, 2142], [899, 2142], [900, 2142], [901, 2142], [902, 2142], [903, 2142]]\n",
      "[[904, 2142], [905, 2142], [906, 2142], [907, 2142], [908, 2142], [909, 2142], [910, 2142], [911, 2142]]\n",
      "[[912, 2142], [913, 2142], [914, 2142], [915, 2142], [916, 2142], [917, 2142], [918, 2142], [919, 2142]]\n",
      "[[920, 2142], [921, 2142], [922, 2142], [923, 2142], [924, 2142], [925, 2142], [926, 2142], [927, 2142]]\n",
      "[[928, 2142], [929, 2142], [930, 2142], [931, 2142], [932, 2142], [933, 2142], [934, 2142], [935, 2142]]\n",
      "[[936, 2142], [937, 2142], [938, 2142], [939, 2142], [940, 2142], [941, 2142], [942, 2142], [943, 2142]]\n",
      "[[944, 2142], [945, 2142], [946, 2142], [947, 2142], [948, 2142], [949, 2142], [950, 2142], [951, 2142]]\n",
      "[[952, 2142], [953, 2142], [954, 2142], [955, 2142], [956, 2142], [957, 2142], [958, 2142], [959, 2142]]\n",
      "[[960, 2142], [961, 2142], [962, 2142], [963, 2142], [964, 2142], [965, 2142], [966, 2142], [967, 2142]]\n",
      "[[968, 2142], [969, 2142], [970, 2142], [971, 2142], [972, 2142], [973, 2142], [974, 2142], [975, 2142]]\n",
      "[[976, 2142], [977, 2142], [978, 2142], [979, 2142], [980, 2142], [981, 2142], [982, 2142], [983, 2142]]\n",
      "[[984, 2142], [985, 2142], [986, 2142], [987, 2142], [988, 2142], [989, 2142], [990, 2142], [991, 2142]]\n",
      "[[992, 2142], [993, 2142], [994, 2142], [995, 2142], [996, 2142], [997, 2142], [998, 2142], [999, 2142]]\n",
      "[[1000, 2142], [1001, 2142], [1002, 2142], [1003, 2142], [1004, 2142], [1005, 2142], [1006, 2142], [1007, 2142]]\n",
      "[[1008, 2142], [1009, 2142], [1010, 2142], [1011, 2142], [1012, 2142], [1013, 2142], [1014, 2142], [1015, 2142]]\n",
      "[[1016, 2142], [1017, 2142], [1018, 2142], [1019, 2142], [1020, 2142], [1021, 2142], [1022, 2142], [1023, 2142]]\n",
      "[[1024, 2142], [1025, 2142], [1026, 2142], [1027, 2142], [1028, 2142], [1029, 2142], [1030, 2142], [1031, 2142]]\n",
      "[[1032, 2142], [1033, 2142], [1034, 2142], [1035, 2142], [1036, 2142], [1037, 2142], [1038, 2142], [1039, 2142]]\n",
      "[[1040, 2142], [1041, 2142], [1042, 2142], [1043, 2142], [1044, 2142], [1045, 2142], [1046, 2142], [1047, 2142]]\n",
      "[[1048, 2142], [1049, 2142], [1050, 2142], [1051, 2142], [1052, 2142], [1053, 2142], [1054, 2142], [1055, 2142]]\n",
      "[[1056, 2142], [1057, 2142], [1058, 2142], [1059, 2142], [1060, 2142], [1061, 2142], [1062, 2142], [1063, 2142]]\n",
      "[[1064, 2142], [1065, 2142], [1066, 2142], [1067, 2142], [1068, 2142], [1069, 2142], [1070, 2142], [1071, 2142]]\n",
      "[[1072, 2142], [1073, 2142], [1074, 2142], [1075, 2142], [1076, 2142], [1077, 2142], [1078, 2142], [1079, 2142]]\n",
      "[[1080, 2142], [1081, 2142], [1082, 2142], [1083, 2142], [1084, 2142], [1085, 2142], [1086, 2142], [1087, 2142]]\n",
      "[[1088, 2142], [1089, 2142], [1090, 2142], [1091, 2142], [1092, 2142], [1093, 2142], [1094, 2142], [1095, 2142]]\n",
      "[[1096, 2142], [1097, 2142], [1098, 2142], [1099, 2142], [1100, 2142], [1101, 2142], [1102, 2142], [1103, 2142]]\n",
      "[[1104, 2142], [1105, 2142], [1106, 2142], [1107, 2142], [1108, 2142], [1109, 2142], [1110, 2142], [1111, 2142]]\n",
      "[[1112, 2142], [1113, 2142], [1114, 2142], [1115, 2142], [1116, 2142], [1117, 2142], [1118, 2142], [1119, 2142]]\n",
      "[[1120, 2142], [1121, 2142], [1122, 2142], [1123, 2142], [1124, 2142], [1125, 2142], [1126, 2142], [1127, 2142]]\n",
      "[[1128, 2142], [1129, 2142], [1130, 2142], [1131, 2142], [1132, 2142], [1133, 2142], [1134, 2142], [1135, 2142]]\n",
      "[[1136, 2142], [1137, 2142], [1138, 2142], [1139, 2142], [1140, 2142], [1141, 2142], [1142, 2142], [1143, 2142]]\n",
      "[[1144, 2142], [1145, 2142], [1146, 2142], [1147, 2142], [1148, 2142], [1149, 2142], [1150, 2142], [1151, 2142]]\n",
      "[[1152, 2142], [1153, 2142], [1154, 2142], [1155, 2142], [1156, 2142], [1157, 2142], [1158, 2142], [1159, 2142]]\n",
      "[[1160, 2142], [1161, 2142], [1162, 2142], [1163, 2142], [1164, 2142], [1165, 2142], [1166, 2142], [1167, 2142]]\n",
      "[[1168, 2142], [1169, 2142], [1170, 2142], [1171, 2142], [1172, 2142], [1173, 2142], [1174, 2142], [1175, 2142]]\n",
      "[[1176, 2142], [1177, 2142], [1178, 2142], [1179, 2142], [1180, 2142], [1181, 2142], [1182, 2142], [1183, 2142]]\n",
      "[[1184, 2142], [1185, 2142], [1186, 2142], [1187, 2142], [1188, 2142], [1189, 2142], [1190, 2142], [1191, 2142]]\n",
      "[[1192, 2142], [1193, 2142], [1194, 2142], [1195, 2142], [1196, 2142], [1197, 2142], [1198, 2142], [1199, 2142]]\n",
      "[[1200, 2142], [1201, 2142], [1202, 2142], [1203, 2142], [1204, 2142], [1205, 2142], [1206, 2142], [1207, 2142]]\n",
      "[[1208, 2142], [1209, 2142], [1210, 2142], [1211, 2142], [1212, 2142], [1213, 2142], [1214, 2142], [1215, 2142]]\n",
      "[[1216, 2142], [1217, 2142], [1218, 2142], [1219, 2142], [1220, 2142], [1221, 2142], [1222, 2142], [1223, 2142]]\n",
      "[[1224, 2142], [1225, 2142], [1226, 2142], [1227, 2142], [1228, 2142], [1229, 2142], [1230, 2142], [1231, 2142]]\n",
      "[[1232, 2142], [1233, 2142], [1234, 2142], [1235, 2142], [1236, 2142], [1237, 2142], [1238, 2142], [1239, 2142]]\n",
      "[[1240, 2142], [1241, 2142], [1242, 2142], [1243, 2142], [1244, 2142], [1245, 2142], [1246, 2142], [1247, 2142]]\n",
      "[[1248, 2142], [1249, 2142], [1250, 2142], [1251, 2142], [1252, 2142], [1253, 2142], [1254, 2142], [1255, 2142]]\n",
      "[[1256, 2142], [1257, 2142], [1258, 2142], [1259, 2142], [1260, 2142], [1261, 2142], [1262, 2142], [1263, 2142]]\n",
      "[[1264, 2142], [1265, 2142], [1266, 2142], [1267, 2142], [1268, 2142], [1269, 2142], [1270, 2142], [1271, 2142]]\n",
      "[[1272, 2142], [1273, 2142], [1274, 2142], [1275, 2142], [1276, 2142], [1277, 2142], [1278, 2142], [1279, 2142]]\n",
      "[[1280, 2142], [1281, 2142], [1282, 2142], [1283, 2142], [1284, 2142], [1285, 2142], [1286, 2142], [1287, 2142]]\n",
      "[[1288, 2142], [1289, 2142], [1290, 2142], [1291, 2142], [1292, 2142], [1293, 2142], [1294, 2142], [1295, 2142]]\n"
     ]
    }
   ],
   "source": [
    "num_ntype = 3\n",
    "dropout_rate = 0.5\n",
    "lr = 0.005\n",
    "weight_decay = 0.001\n",
    "hidden_dim=64\n",
    "num_heads=1\n",
    "attn_vec_dim=128\n",
    "neighbor_samples=100\n",
    "batch_size=8\n",
    "#etypes_lists = [[[0, 1],[0, 2, 3, 1]],\n",
    " #               [[1, 0],[2, 3]]\n",
    "etypes_lists = [[[0, 1],[0, 2, 3, 1],[4,None,5]],\n",
    "                [[1, 0],[2, 3],[2, None, 3]]]\n",
    "use_masks = [[True, True],\n",
    "             [True, False]]\n",
    "no_masks = [[False] * 3, [False] * 3]\n",
    "#use_masks = [[True, True, False],\n",
    "#             [True, False, True]]\n",
    "#no_masks = [[False] * 3, [False] * 3]\n",
    "num_mir=1296\n",
    "num_disease=11783\n",
    "num_gene=10116\n",
    "expected_metapaths = [\n",
    "   [(0, 1, 0), (0, 1, 2, 1, 0), (0,2,2,0)],\n",
    "    [(1, 0, 1), (1, 2, 1), (1, 2, 2, 1)]\n",
    "]\n",
    "features_list = []\n",
    "in_dims = []\n",
    "\n",
    "adjlists_ua, edge_metapath_indices_list_ua, _, type_mask, train_val_test_pos_user_artist, train_val_test_neg_user_artist = load_LastFM_data()\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "for i in range(num_ntype):\n",
    "    dim = (type_mask == i).sum()\n",
    "    in_dims.append(dim)\n",
    "    indices = np.vstack((np.arange(dim), np.arange(dim)))\n",
    "    indices = torch.LongTensor(indices)\n",
    "    values = torch.FloatTensor(np.ones(dim))\n",
    "    features_list.append(torch.sparse.FloatTensor(indices, values, torch.Size([dim, dim])).to(device))\n",
    "net = MEAHNE_lp(\n",
    "            [3, 3], 6, etypes_lists, in_dims, hidden_dim, hidden_dim, num_heads, attn_vec_dim, dropout_rate)\n",
    "net.to(device)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "pred_idx_generator = index_generator(batch_size=batch_size, num_data=len(val_2142), shuffle=False)\n",
    "net.load_state_dict(torch.load('checkpoint/checkpoint_{}.pt'.format('ours')))\n",
    "net.eval()\n",
    "proba_list=[]\n",
    "with torch.no_grad():\n",
    "    for iteration in range(pred_idx_generator.num_iterations()):\n",
    "        pred_idx_batch = pred_idx_generator.next()\n",
    "        pred_batch =val_2142[pred_idx_batch].tolist()\n",
    "        print(pred_batch)\n",
    "        pred_g_lists, pred_indices_lists, pred_idx_batch_mapped_lists,pred_pos_node_lists = parse_minibatch_LastFM(\n",
    "          adjlists_ua, edge_metapath_indices_list_ua, pred_batch, device, neighbor_samples, no_masks, num_mir)\n",
    "        [embedding_mir, embedding_disease], _ = net(\n",
    "                    (pred_g_lists, features_list, type_mask, pred_indices_lists, pred_idx_batch_mapped_lists,pred_pos_node_lists))\n",
    "        embedding_mir = embedding_mir.view(-1, 1, embedding_mir.shape[1])\n",
    "        embedding_disease = embedding_disease.view(-1, embedding_disease.shape[1], 1)\n",
    "        out = torch.bmm(embedding_mir, embedding_disease).flatten()\n",
    "        out = out.cpu().numpy()\n",
    "        proba_list.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hairy-argument",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 2147],\n",
       "       [   1, 2147],\n",
       "       [   2, 2147],\n",
       "       ...,\n",
       "       [1293, 2147],\n",
       "       [1294, 2147],\n",
       "       [1295, 2147]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_2147=[]\n",
    "for i in range(1296):\n",
    "    cur=[]\n",
    "    cur.append(i)\n",
    "    cur.append(2147)\n",
    "    val_2147.append(cur)\n",
    "val_2147=np.array(val_2147)\n",
    "val_2147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ordinary-prediction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -10.02048302, 1288.        ],\n",
       "       [  -8.88030815,  760.        ],\n",
       "       [  -8.87591743,   40.        ],\n",
       "       ...,\n",
       "       [   1.83678842,  178.        ],\n",
       "       [   1.85946429,  244.        ],\n",
       "       [   2.15600204,  240.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_2147_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fleet-reliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2.3979073 ,  2.6384914 ,  2.385874  , -1.1157149 ,  2.3095143 ,\n",
       "         2.359257  ,  0.75443554,  0.74556327], dtype=float32),\n",
       " array([-0.169123 ,  2.1320922,  1.7925895, -1.0215013,  2.0148087,\n",
       "         0.9294625,  1.7588209, -1.4702307], dtype=float32),\n",
       " array([-1.557161  ,  1.6051172 ,  1.2768508 ,  0.42727768, -1.9902754 ,\n",
       "        -2.0212784 ,  0.68829894,  1.7796688 ], dtype=float32),\n",
       " array([ 2.4370763, -1.7714345,  2.1420822,  1.5901086,  2.2458584,\n",
       "         0.3289292, -0.8008124, -0.8323271], dtype=float32),\n",
       " array([ 0.6409991 , -1.5067164 , -1.4473978 , -1.7148634 ,  0.26868007,\n",
       "        -0.57627904, -0.98748195, -0.4290437 ], dtype=float32),\n",
       " array([-1.3708684 , -0.7397567 ,  0.23112126, -1.0679251 ,  0.46382815,\n",
       "        -0.95484376, -1.037783  , -0.6369655 ], dtype=float32),\n",
       " array([ 0.6710872 ,  3.0323694 ,  0.61476684,  0.579403  ,  0.6012489 ,\n",
       "         0.604111  ,  0.6238135 , -0.1450986 ], dtype=float32),\n",
       " array([-2.0566864 ,  0.40542668, -1.7267702 , -1.7844185 , -1.7549169 ,\n",
       "         0.4999383 ,  0.29897594, -0.886635  ], dtype=float32),\n",
       " array([-0.30880666,  2.7342873 ,  2.2892728 ,  2.408627  ,  1.8597281 ,\n",
       "        -0.7554872 , -0.31940782, -1.370215  ], dtype=float32),\n",
       " array([-1.800594 ,  1.4161897,  0.746004 ,  0.5153936, -1.9995679,\n",
       "         1.6465695, -1.8140397, -1.8386165], dtype=float32),\n",
       " array([-1.4380326 ,  1.4236172 , -1.1063151 , -0.41209823,  0.52416134,\n",
       "        -1.5598483 , -0.97302425, -1.1873889 ], dtype=float32),\n",
       " array([ 1.251658 ,  2.7421908, -1.5876294, -1.685737 ,  2.2810786,\n",
       "         2.2760508,  2.764956 ,  3.1203425], dtype=float32),\n",
       " array([ 0.1736062 ,  1.6595474 ,  0.43214983, -1.4951239 , -0.88402504,\n",
       "        -1.6888165 ,  0.2984455 , -0.45221525], dtype=float32),\n",
       " array([-0.65074664,  0.7664529 ,  0.4574737 , -0.661017  ,  0.95596844,\n",
       "        -0.5380402 ,  0.6169648 ,  2.261744  ], dtype=float32),\n",
       " array([-1.4383794,  0.4144949,  1.0274746, -0.7479914, -0.9702791,\n",
       "        -0.8947432, -1.4552226,  0.9250143], dtype=float32),\n",
       " array([ 0.4400448 ,  0.47617212,  1.0042741 , -0.90941626, -0.958096  ,\n",
       "         0.9834355 ,  0.7221019 ,  0.44173357], dtype=float32),\n",
       " array([ 0.8490742 ,  0.72434217, -0.46492165,  0.34524584, -1.43483   ,\n",
       "        -1.4097066 , -1.586933  , -1.6303946 ], dtype=float32),\n",
       " array([-0.43486077,  0.6069321 ,  0.04241705,  0.5385981 ,  0.56007797,\n",
       "         2.311497  ,  1.0887738 ,  1.9526792 ], dtype=float32),\n",
       " array([-1.3665562 ,  0.8123245 ,  0.78467035, -0.51372117,  0.38302487,\n",
       "        -0.2640585 , -0.5829024 ,  0.62368363], dtype=float32),\n",
       " array([-1.3465838 ,  0.63807756,  1.1845886 , -0.4572053 , -0.90194684,\n",
       "        -1.0566242 ,  1.7512556 , -0.89616424], dtype=float32),\n",
       " array([ 0.49250725,  0.27834287, -1.0801053 , -1.1537039 , -0.83977425,\n",
       "        -0.8732294 , -1.0204277 , -1.0895274 ], dtype=float32),\n",
       " array([-0.5577626 , -0.6309583 ,  0.89563704, -1.4904616 , -0.32953838,\n",
       "        -1.4538597 ,  0.81820315, -0.8412106 ], dtype=float32),\n",
       " array([ 2.431453  ,  1.9754536 ,  3.18784   , -0.79362595,  0.5501261 ,\n",
       "         0.699645  , -0.93672746,  1.9609919 ], dtype=float32),\n",
       " array([ 2.5341582 ,  1.8288587 ,  2.2460058 ,  2.5435488 ,  2.2331288 ,\n",
       "        -0.53372586,  0.5542726 ,  1.2681012 ], dtype=float32),\n",
       " array([1.9954416, 2.4618084, 2.0807862, 2.2292466, 2.8654528, 2.4866543,\n",
       "        0.9053553, 0.851648 ], dtype=float32),\n",
       " array([ 2.625672 , -1.8092911,  3.1251922, -1.4394776,  2.7449677,\n",
       "        -1.4226493,  2.765955 , -1.4119285], dtype=float32),\n",
       " array([ 0.3601279,  2.6134844, -1.4861149,  2.2563398,  2.7149565,\n",
       "        -1.4684842, -1.3695141,  2.0075989], dtype=float32),\n",
       " array([-1.7755737 , -1.6072476 ,  2.8353937 ,  2.9092946 , -1.470415  ,\n",
       "         0.27200872,  0.41056022,  0.5896387 ], dtype=float32),\n",
       " array([0.3895918 , 0.46604237, 2.3601444 , 2.258527  , 2.281736  ,\n",
       "        0.5724661 , 2.8824444 , 0.57840735], dtype=float32),\n",
       " array([ 0.6816489 ,  0.33319446,  1.7875836 ,  1.4697938 ,  0.40660772,\n",
       "         0.3437357 , -0.63325185,  2.101263  ], dtype=float32),\n",
       " array([ 3.5246544 , -1.4602445 , -0.29839525, -0.88266385,  3.1937685 ,\n",
       "         2.4116964 ,  3.1989253 ,  2.0667515 ], dtype=float32),\n",
       " array([ 1.7821972 , -0.78126705, -0.84263945, -0.01637113,  3.1870897 ,\n",
       "        -1.5238558 , -1.4682374 ,  2.1088765 ], dtype=float32),\n",
       " array([ 1.6025331, -1.1869986,  1.7508941,  0.5237861,  1.8668365,\n",
       "         1.9821378,  1.4523721,  1.5679984], dtype=float32),\n",
       " array([-1.7286415 ,  0.53730047,  0.35148132,  2.4737585 , -1.937385  ,\n",
       "         0.36609393, -2.1846662 ,  0.24048471], dtype=float32),\n",
       " array([ 2.3707638 , -1.4690716 ,  0.5580294 ,  0.09237572,  1.9743488 ,\n",
       "         1.4607073 ,  1.2241962 ,  0.41102818], dtype=float32),\n",
       " array([-1.3844078 , -1.5133402 , -1.3113111 ,  2.2755947 ,  1.9364566 ,\n",
       "         1.1196151 ,  0.32606965,  1.3558556 ], dtype=float32),\n",
       " array([ 0.1319934 ,  0.71148044,  0.5246649 ,  2.1014585 , -0.3261792 ,\n",
       "         0.28358215,  0.67715645,  0.09777856], dtype=float32),\n",
       " array([ 2.2939343 , -0.46399146, -1.5040734 ,  0.6531888 ,  2.3631732 ,\n",
       "        -1.5251172 , -1.5579346 ,  2.749848  ], dtype=float32),\n",
       " array([ 2.2181616 ,  1.1679626 ,  1.1568733 ,  2.4730096 , -1.5299728 ,\n",
       "         0.78518564,  2.6599128 ,  1.9778538 ], dtype=float32),\n",
       " array([ 2.3866663 ,  2.7736087 ,  2.1957653 ,  0.59142226, -1.0861822 ,\n",
       "         0.01626667, -0.9184779 ,  0.42476305], dtype=float32),\n",
       " array([ 1.4032003,  0.6248983,  1.7813964,  1.8901638,  2.1086495,\n",
       "        -1.7619662, -1.6483299,  2.3728116], dtype=float32),\n",
       " array([-1.495202 , -1.4334329,  2.4505143, -1.3700376,  2.5172472,\n",
       "         2.4313674,  2.4231505, -1.899576 ], dtype=float32),\n",
       " array([ 2.3805656,  2.8043492, -1.642982 , -1.6275356,  2.6502385,\n",
       "        -1.6280354,  2.318536 , -1.7163116], dtype=float32),\n",
       " array([ 1.3769125 , -1.7397951 ,  2.1568544 ,  0.35459343,  0.3239832 ,\n",
       "         2.312235  , -1.7636497 ,  2.5792902 ], dtype=float32),\n",
       " array([-1.6791124 ,  2.585751  ,  0.32596502,  2.0422406 ,  0.33149093,\n",
       "        -0.70256853,  2.5656168 , -1.7394365 ], dtype=float32),\n",
       " array([ 2.1622381 , -1.4610968 ,  3.911149  , -1.6065618 ,  3.4009707 ,\n",
       "         1.7155111 ,  0.9265973 , -0.76353884], dtype=float32),\n",
       " array([-1.9960288,  1.6893051,  2.5872166,  2.1147301,  0.7234457,\n",
       "        -0.9732628,  1.2508966,  1.8088237], dtype=float32),\n",
       " array([ 2.2465515,  2.7901468,  1.8884735,  2.213258 , -0.336928 ,\n",
       "         1.9293416,  1.6234908,  1.931912 ], dtype=float32),\n",
       " array([-0.28487074,  0.38982248, -1.5710588 ,  2.3997316 , -0.5173665 ,\n",
       "         3.1747503 , -1.625831  ,  2.8817313 ], dtype=float32),\n",
       " array([ 2.8776035 , -1.6805551 , -2.266429  ,  1.7647629 ,  0.34730682,\n",
       "        -0.6043011 , -2.0762935 , -1.288891  ], dtype=float32),\n",
       " array([2.6626728, 2.3452744, 0.5877748, 1.7684017, 2.0296564, 2.2875853,\n",
       "        2.4585006, 0.5681814], dtype=float32),\n",
       " array([-0.3887494 ,  2.7934632 ,  1.6540837 ,  1.410084  ,  2.80138   ,\n",
       "         0.49550852,  2.7144117 , -1.5968901 ], dtype=float32),\n",
       " array([ 1.5257018 ,  1.0853608 , -2.2421236 , -1.8849399 , -0.0655337 ,\n",
       "         1.6402357 , -0.06423023, -2.224661  ], dtype=float32),\n",
       " array([ 1.7937758 , -1.6122102 ,  0.59941727,  0.47193125,  0.6807109 ,\n",
       "        -1.1067482 ,  2.5812206 ,  2.0240037 ], dtype=float32),\n",
       " array([ 2.0464172 ,  2.229057  ,  2.7239347 ,  0.43114364,  2.7343912 ,\n",
       "         0.60714024, -1.4542755 ,  0.35051116], dtype=float32),\n",
       " array([1.7530674 , 0.37952483, 0.43463624, 1.5043573 , 0.38291794,\n",
       "        0.94015867, 0.3313043 , 0.36158282], dtype=float32),\n",
       " array([ 0.6949672 , -1.3398451 ,  0.69988906, -1.0105717 ,  2.5813508 ,\n",
       "        -1.4372603 , -1.4926813 ,  2.2539914 ], dtype=float32),\n",
       " array([ 2.2468612,  0.8926186,  0.5030852, -1.6816885,  1.2363144,\n",
       "         1.439685 , -1.5822148,  2.3928888], dtype=float32),\n",
       " array([-1.9765741 , -0.42352927, -1.565418  , -1.5357134 , -1.2874498 ,\n",
       "        -0.8446142 , -0.79840577, -0.81705326], dtype=float32),\n",
       " array([ 0.09899616, -0.59964967,  0.03149505, -0.8866576 , -0.7650371 ,\n",
       "        -1.2583675 ,  0.4653195 ,  0.39441568], dtype=float32),\n",
       " array([-1.2340845 ,  0.35503805, -0.904304  , -1.3324674 ,  0.46931288,\n",
       "        -1.0472639 ,  0.5790104 ,  1.2574826 ], dtype=float32),\n",
       " array([-0.37216598, -1.2176621 ,  0.50830036,  0.5647327 ,  0.45688528,\n",
       "        -0.7018084 ,  0.49551582, -0.99130404], dtype=float32),\n",
       " array([-0.3417195 , -0.32543272, -0.24054933, -0.34904787, -1.2609216 ,\n",
       "        -1.1941411 , -1.0896095 ,  0.52998054], dtype=float32),\n",
       " array([-0.49804765,  0.41947836, -0.9318681 , -0.3640002 ,  0.4749777 ,\n",
       "         0.08332862, -0.39975947, -0.46325213], dtype=float32),\n",
       " array([ 1.9670358 ,  0.5797735 , -2.1153874 , -0.46457702,  0.70791614,\n",
       "         0.8399099 ,  0.86349416,  0.669537  ], dtype=float32),\n",
       " array([0.89101696, 0.7538087 , 0.75192523, 0.581196  , 0.9665951 ,\n",
       "        0.355838  , 0.80863833, 2.2295978 ], dtype=float32),\n",
       " array([-1.501806  , -1.6616971 , -1.2386336 ,  1.9225007 , -0.8384501 ,\n",
       "        -0.87138826,  1.7363247 ,  0.5025091 ], dtype=float32),\n",
       " array([-0.3674419 , -0.22955841,  2.5603178 ,  2.3467925 ,  1.4721138 ,\n",
       "        -1.4170291 ,  2.4417944 , -1.4185581 ], dtype=float32),\n",
       " array([-0.23663013,  0.556329  , -1.6617887 ,  1.7024102 , -1.6397893 ,\n",
       "        -1.6370211 ,  0.7551699 , -1.6631634 ], dtype=float32),\n",
       " array([ 2.7932088,  1.4574744,  2.150385 ,  1.947998 ,  2.2447538,\n",
       "        -1.5708597, -1.3844008,  0.8294148], dtype=float32),\n",
       " array([ 1.8760443,  2.8388913, -1.615784 ,  2.802968 ,  2.3475988,\n",
       "        -1.6102574, -1.8629708, -1.4713206], dtype=float32),\n",
       " array([ 0.5971992 , -0.19730882, -0.8360853 , -1.082342  , -1.8282622 ,\n",
       "         0.44111073, -1.4171238 , -1.1216527 ], dtype=float32),\n",
       " array([-0.07254851, -1.5901791 ,  2.4153898 , -1.2651974 , -0.63052213,\n",
       "        -0.28137076,  0.7630653 ,  0.16764332], dtype=float32),\n",
       " array([-0.8319082 ,  1.6491578 , -1.6115017 , -1.0317587 ,  0.23821759,\n",
       "        -0.5178399 ,  2.2765775 , -0.8723197 ], dtype=float32),\n",
       " array([ 0.50183433, -1.1616005 ,  1.271016  ,  0.29685965,  0.553602  ,\n",
       "        -0.61596656, -0.9017974 , -0.34335274], dtype=float32),\n",
       " array([-0.57660943, -1.8770233 ,  0.30693084,  0.22625718, -0.36593705,\n",
       "        -1.666516  , -2.1641228 ,  0.31650555], dtype=float32),\n",
       " array([-1.4093434 ,  1.1769434 , -1.950692  ,  0.23805991, -0.59742266,\n",
       "        -1.064081  , -0.93117464, -0.9087373 ], dtype=float32),\n",
       " array([ 0.34759736, -0.5863348 ,  0.6464476 ,  2.1860666 , -1.6906525 ,\n",
       "         0.59882694, -0.25422817,  0.36845943], dtype=float32),\n",
       " array([ 0.37832338,  2.2536032 ,  1.6702667 , -1.6335167 , -1.2003983 ,\n",
       "         0.66093063,  0.7171681 , -1.1337047 ], dtype=float32),\n",
       " array([2.5251937 , 0.4454297 , 0.62042445, 0.4136538 , 0.4069676 ,\n",
       "        1.0404755 , 0.33500502, 0.3457861 ], dtype=float32),\n",
       " array([ 2.1246893 ,  1.6661406 ,  0.5843174 ,  0.4042085 , -0.446517  ,\n",
       "         0.42905456,  0.34278363,  0.4362933 ], dtype=float32),\n",
       " array([ 0.6590002 ,  0.7706654 ,  0.74613625,  0.67640007,  2.2829196 ,\n",
       "         0.38937652, -1.681543  ,  1.231951  ], dtype=float32),\n",
       " array([ 1.8999698 ,  1.9592537 ,  0.66389626, -1.6343157 , -1.3227631 ,\n",
       "        -1.4250305 ,  0.3456169 , -0.05394889], dtype=float32),\n",
       " array([-0.53720456, -1.1326495 , -1.0237871 ,  0.09900378,  0.09914768,\n",
       "         0.22546583, -1.1336505 , -1.1013963 ], dtype=float32),\n",
       " array([-0.3502743 ,  0.6132824 , -0.69605875,  1.0308255 , -1.4287668 ,\n",
       "         1.6176358 ,  1.5603607 ,  0.42874944], dtype=float32),\n",
       " array([ 0.3515839 ,  1.0716792 ,  0.21821749,  2.1402428 , -1.8758024 ,\n",
       "         2.1764262 , -1.4831965 ,  2.1824212 ], dtype=float32),\n",
       " array([-1.9552268 , -1.6353279 ,  0.12751012, -0.56282336,  0.32435608,\n",
       "        -0.53952307, -0.99371064,  0.08523557], dtype=float32),\n",
       " array([-1.7026429 , -1.4383721 , -0.46083704,  0.70540184,  2.3759005 ,\n",
       "        -0.3939915 , -1.4014505 , -1.6065936 ], dtype=float32),\n",
       " array([-1.0001917 , -1.0183786 , -1.0369482 ,  0.09599102,  0.20232973,\n",
       "         0.23774946,  1.0410799 , -1.1243842 ], dtype=float32),\n",
       " array([-2.043402 , -1.5571079,  1.386917 , -0.8885976, -0.5315996,\n",
       "         1.2505181, -0.4652459, -1.7369336], dtype=float32),\n",
       " array([-0.9261085 ,  0.40962842, -1.0635073 , -1.1343888 , -1.073721  ,\n",
       "         0.45742744,  0.2261663 ,  0.16234304], dtype=float32),\n",
       " array([ 0.4464944 ,  0.52161634, -0.8133533 , -0.87002885, -0.9729539 ,\n",
       "        -1.0742042 , -0.8737662 ,  0.27938318], dtype=float32),\n",
       " array([-1.2630705 ,  0.23490897, -0.73093075,  0.43287677,  1.473971  ,\n",
       "        -0.92788494, -0.9771459 ,  0.52241766], dtype=float32),\n",
       " array([-0.85297483,  0.31429553,  0.42700344,  0.2745467 , -0.46702266,\n",
       "        -1.1775528 ,  2.2696383 ,  1.8072885 ], dtype=float32),\n",
       " array([ 0.4705638 ,  0.08173725, -0.5203712 , -0.5619832 ,  0.27512297,\n",
       "        -1.2244829 , -0.84978646, -0.9736064 ], dtype=float32),\n",
       " array([-1.2077193 , -0.8816372 ,  0.8489687 ,  2.725902  , -0.36414367,\n",
       "        -0.5079634 ,  0.21086611, -0.7034104 ], dtype=float32),\n",
       " array([-0.8074702 ,  1.790752  , -1.2540007 ,  0.70539546,  0.4990768 ,\n",
       "        -1.7751656 , -1.7471583 , -1.1403702 ], dtype=float32),\n",
       " array([ 0.752152  , -1.0976461 ,  1.8135306 ,  2.3319907 ,  1.4511199 ,\n",
       "         0.6158105 , -0.5065502 ,  0.77439594], dtype=float32),\n",
       " array([-1.4358492 , -0.93032527, -1.6430969 , -1.4591804 , -1.4936373 ,\n",
       "        -1.7211776 , -0.65306807, -1.3929703 ], dtype=float32),\n",
       " array([-1.1100624 ,  0.43827024, -0.9014417 , -0.942201  , -0.8904855 ,\n",
       "        -0.9010593 , -1.0398406 , -0.6950861 ], dtype=float32),\n",
       " array([ 0.32980624,  0.40645903, -0.7601682 , -1.308322  , -1.2345119 ,\n",
       "        -0.9725022 ,  0.3499074 , -1.046763  ], dtype=float32),\n",
       " array([-1.8240443 , -1.240701  ,  0.49942598, -0.96672904,  0.4486718 ,\n",
       "         0.4135656 ,  0.40934515, -0.98286587], dtype=float32),\n",
       " array([-0.43056855, -1.3337622 , -1.1421511 , -0.91236484,  0.4184329 ,\n",
       "        -1.0586481 , -1.2509366 ,  0.41364443], dtype=float32),\n",
       " array([-1.3667973 , -0.32906926, -1.5688603 , -0.94015104, -1.1088927 ,\n",
       "        -0.9655723 ,  0.26162118, -1.1241974 ], dtype=float32),\n",
       " array([ 2.4595327 , -1.5150185 ,  1.2918459 ,  1.9898514 , -1.3691175 ,\n",
       "         2.6838632 , -1.4424832 , -0.15060374], dtype=float32),\n",
       " array([ 1.6468765 ,  0.7261738 ,  0.790099  ,  2.1164834 , -1.4689157 ,\n",
       "         1.5826886 , -1.4785203 ,  0.88651085], dtype=float32),\n",
       " array([0.47787857, 1.8221946 , 1.900375  , 0.42648944, 1.6504604 ,\n",
       "        1.6835688 , 2.3250206 , 1.6269177 ], dtype=float32),\n",
       " array([ 0.45172256,  0.4722466 , -1.663295  , -1.83111   , -2.0950642 ,\n",
       "        -1.5157915 ,  0.41970512, -2.0394516 ], dtype=float32),\n",
       " array([ 0.96867883, -1.3707901 ,  0.77014756, -1.3935443 , -1.6601253 ,\n",
       "         1.6920264 ,  0.62353814,  0.94651383], dtype=float32),\n",
       " array([ 2.2757692 ,  0.8260471 ,  0.90019655, -1.3356826 , -1.3290296 ,\n",
       "         1.0927382 ,  0.15476127,  0.23451957], dtype=float32),\n",
       " array([ 0.36414126, -1.5580263 , -1.8444762 , -1.5118749 , -1.0644687 ,\n",
       "         0.31433785, -0.16251107,  0.5264413 ], dtype=float32),\n",
       " array([ 0.46713907,  0.51224387,  0.48714393, -1.9558048 ,  0.31410584,\n",
       "        -1.9920485 , -1.5522466 ,  0.24446148], dtype=float32),\n",
       " array([-0.04540336,  0.52879775,  0.525374  , -1.6418025 , -1.566133  ,\n",
       "         0.3488258 ,  0.47343665, -1.5626963 ], dtype=float32),\n",
       " array([-1.305076 , -1.3774567, -1.3081036, -1.2516763, -0.6184949,\n",
       "        -1.5142308, -1.1684574, -1.0305998], dtype=float32),\n",
       " array([-0.72749865, -1.0151889 , -0.34408987, -0.9426445 , -1.2947738 ,\n",
       "        -0.9237073 , -0.85937375, -0.7671304 ], dtype=float32),\n",
       " array([ 0.55979747,  1.1267594 , -0.4759711 , -0.53914124, -0.7734905 ,\n",
       "        -0.7002758 , -0.463947  ,  0.9118468 ], dtype=float32),\n",
       " array([-1.6176236 , -1.4002033 ,  0.5419637 , -0.9515481 , -0.92763346,\n",
       "         0.41205978, -1.6941803 , -1.4484173 ], dtype=float32),\n",
       " array([-1.5225518 ,  0.34852362,  0.16315597,  0.2733993 ,  0.3505694 ,\n",
       "         0.88895684,  0.7691086 ,  0.4407784 ], dtype=float32),\n",
       " array([-0.12604903,  0.8852129 ,  0.7223157 ,  0.7017654 ,  0.73372376,\n",
       "        -1.3092307 ,  0.694848  ,  0.7840641 ], dtype=float32),\n",
       " array([ 0.7332681 ,  0.5099746 , -2.090822  , -2.1065438 , -2.127853  ,\n",
       "         1.6863261 ,  0.34119332,  0.41983312], dtype=float32),\n",
       " array([-1.6380332 ,  0.40042043, -0.39554164, -1.8269384 , -1.8859086 ,\n",
       "         0.47086084,  0.5047361 , -1.1445603 ], dtype=float32),\n",
       " array([ 2.1667497 , -1.2626275 ,  0.58869505, -1.0451621 ,  1.7356956 ,\n",
       "        -1.5428373 , -1.8341326 ,  0.5945234 ], dtype=float32),\n",
       " array([-0.6841199 ,  0.72764677,  0.46718067,  0.47322243,  0.24380311,\n",
       "         0.21099581,  0.19915144,  0.22710118], dtype=float32),\n",
       " array([ 0.23718964,  0.2431165 ,  0.30653965, -1.3291919 ,  0.26523083,\n",
       "        -1.1658466 , -0.7818541 ,  0.5450559 ], dtype=float32),\n",
       " array([ 0.9058516 , -1.1750526 ,  0.75658435,  0.7976484 ,  0.78112274,\n",
       "        -1.2501827 , -0.51231825, -0.6392929 ], dtype=float32),\n",
       " array([-1.1214772 ,  0.47474077,  0.3007809 , -0.8772599 ,  0.36955592,\n",
       "        -0.4885087 , -0.8698672 , -1.102724  ], dtype=float32),\n",
       " array([-0.11696829,  0.5560126 ,  0.50541675,  0.50806963,  0.6008513 ,\n",
       "        -0.01086789,  0.02047057, -0.93483967], dtype=float32),\n",
       " array([ 0.9712273 ,  0.7762614 ,  0.9637918 , -0.1604829 ,  0.6088021 ,\n",
       "         0.37155932,  0.74322504,  0.7829366 ], dtype=float32),\n",
       " array([-1.2424353 , -1.4413989 , -1.7258263 , -1.3005354 ,  0.23829983,\n",
       "        -1.3144715 , -1.2810643 ,  0.298607  ], dtype=float32),\n",
       " array([-0.880793  ,  0.45399022,  0.4352097 , -1.1313537 ,  0.35832477,\n",
       "        -1.0153165 , -0.88487285,  0.4507509 ], dtype=float32),\n",
       " array([ 0.59929955,  0.5868818 ,  0.6214333 ,  1.8046706 , -1.498169  ,\n",
       "        -1.3223789 ,  1.4228212 ,  0.8214398 ], dtype=float32),\n",
       " array([ 0.9564714 , -0.8369296 ,  0.9083101 , -0.5679578 ,  1.9577386 ,\n",
       "        -0.96493495, -0.9577202 ,  0.03670777], dtype=float32),\n",
       " array([ 1.6331164 ,  0.68247974, -0.1360296 , -1.108618  , -0.7633972 ,\n",
       "         0.64359474,  2.3251483 , -1.2593306 ], dtype=float32),\n",
       " array([-1.2891791 ,  1.8194745 ,  0.716631  ,  0.5932945 ,  0.10039395,\n",
       "         0.58966506,  0.7882535 ,  0.8517735 ], dtype=float32),\n",
       " array([ 0.5131469 ,  0.5109018 ,  1.2617682 ,  0.8592839 , -2.039878  ,\n",
       "         0.45148206, -2.0640926 , -0.32360244], dtype=float32),\n",
       " array([-1.0146363 , -1.0455632 ,  2.1230216 ,  0.4780039 , -0.7379724 ,\n",
       "        -0.6065602 ,  0.35185614,  0.61384577], dtype=float32),\n",
       " array([ 0.8479501 ,  1.355086  , -1.3885978 ,  0.88433784,  0.08898062,\n",
       "        -0.14562243, -0.06596416, -0.9117346 ], dtype=float32),\n",
       " array([-1.4084802 , -1.0832932 ,  0.9025088 , -0.2825008 ,  0.840409  ,\n",
       "         0.7707215 ,  0.74956864, -0.8542985 ], dtype=float32),\n",
       " array([ 1.0944182 , -1.3046297 ,  0.47402188,  0.87667245, -1.4459308 ,\n",
       "        -0.21417665, -0.99084914,  0.81327987], dtype=float32),\n",
       " array([-1.6084511 ,  0.6081878 ,  1.2968459 ,  0.5226916 , -0.8882587 ,\n",
       "        -0.9611735 ,  0.24817352, -0.8335086 ], dtype=float32),\n",
       " array([ 0.65084386, -1.5324028 ,  0.597829  ,  0.5532612 ,  1.8944759 ,\n",
       "         0.47835624,  0.5812198 , -0.3155715 ], dtype=float32),\n",
       " array([-1.4896504 ,  0.47155088, -1.1231292 , -1.2476181 , -0.71884537,\n",
       "        -1.4299592 , -1.4854674 ,  0.5318427 ], dtype=float32),\n",
       " array([ 0.7950535 , -1.8370486 ,  0.33867967, -0.4337406 , -1.4869394 ,\n",
       "         1.2520406 , -0.7939703 , -0.7008766 ], dtype=float32),\n",
       " array([ 1.6091583 ,  0.1568479 , -0.26846433, -0.4050195 ,  0.10883648,\n",
       "        -1.2123803 , -0.43055445,  0.97370416], dtype=float32),\n",
       " array([ 0.4219843 , -1.4655423 ,  2.056176  ,  0.50397867,  0.4275521 ,\n",
       "         0.330131  , -0.20855531, -0.59319687], dtype=float32),\n",
       " array([ 0.59524906, -1.6650119 , -1.7862031 ,  0.6471281 ,  0.04237182,\n",
       "        -1.574434  ,  0.5402016 , -0.85305035], dtype=float32),\n",
       " array([-0.5642378 , -1.4356682 ,  0.01256996,  0.57496387, -1.558239  ,\n",
       "        -1.8331256 , -1.2474761 ,  0.34985775], dtype=float32),\n",
       " array([-0.6786925, -1.197351 , -1.2754017, -1.5196863, -1.3784821,\n",
       "        -1.2652638,  0.2524094, -1.3830298], dtype=float32),\n",
       " array([-1.6728952 , -0.33878514, -1.0059919 , -0.427181  , -0.6759898 ,\n",
       "        -1.1280622 , -0.9794756 , -1.624063  ], dtype=float32),\n",
       " array([-0.5346091, -1.3314356, -0.9963428, -0.7156291,  2.0011368,\n",
       "         1.6350657,  2.1069934,  1.8412924], dtype=float32),\n",
       " array([-1.7970914 , -1.1494354 ,  0.05739745,  1.8894922 ,  0.17499882,\n",
       "         0.1437535 , -1.195682  , -1.0932941 ], dtype=float32),\n",
       " array([-0.22700575, -1.4747946 , -1.3370216 , -1.0467281 ,  0.33872676,\n",
       "        -1.0889983 ,  1.3997065 , -1.1580154 ], dtype=float32),\n",
       " array([-0.53355044,  0.6049593 , -1.136138  ,  1.818267  ,  0.97781926,\n",
       "        -0.74511963, -0.56927496, -0.7363033 ], dtype=float32),\n",
       " array([-1.1280314 ,  0.5307109 ,  0.63715357,  0.6595472 ,  0.6320849 ,\n",
       "         0.6613464 , -0.06084743, -0.8458992 ], dtype=float32),\n",
       " array([-0.8242825 , -1.0229822 , -0.9958023 , -0.2632779 , -0.9892004 ,\n",
       "        -0.15890214, -0.999376  ,  0.24051961], dtype=float32),\n",
       " array([ 0.1432463 , -1.1519101 ,  0.32678455,  1.4643829 , -1.209118  ,\n",
       "         0.1818829 ,  0.33376122,  0.7626546 ], dtype=float32),\n",
       " array([-1.3823609 ,  1.5801843 ,  0.7667284 ,  0.5295806 ,  0.38862038,\n",
       "         1.222599  , -0.588474  ,  2.4010262 ], dtype=float32),\n",
       " array([ 2.1065078 ,  1.6426132 ,  2.017559  , -1.6361752 ,  0.35907674,\n",
       "         2.128739  , -1.6675606 , -1.8724908 ], dtype=float32),\n",
       " array([ 0.58054954, -1.5268651 ,  1.9272306 ,  1.5408189 ,  1.3592732 ,\n",
       "        -1.4918666 ,  1.9640825 ,  1.9167837 ], dtype=float32),\n",
       " array([-1.7643211 ,  0.5554049 ,  0.48549777,  1.1468586 ,  0.7988264 ,\n",
       "         1.4734684 ,  0.36903748, -0.3442345 ], dtype=float32),\n",
       " array([-0.55024767, -0.839519  , -0.7098656 ,  0.6046048 , -0.97576904,\n",
       "         0.7098086 ,  1.8678225 , -0.6363718 ], dtype=float32),\n",
       " array([-2.4558349,  2.2050738,  2.1220348,  0.3879932,  1.9325624,\n",
       "         1.9971764, -1.7410953, -0.0180279], dtype=float32)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "located-astrology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 2142],\n",
       "       [   1, 2142],\n",
       "       [   2, 2142],\n",
       "       ...,\n",
       "       [1293, 2142],\n",
       "       [1294, 2142],\n",
       "       [1295, 2142]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_2142=[]\n",
    "for i in range(1296):\n",
    "    cur=[]\n",
    "    cur.append(i)\n",
    "    cur.append(2142)\n",
    "    val_2142.append(cur)\n",
    "val_2142=np.array(val_2142)\n",
    "val_2142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "square-sherman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[240.0,\n",
       " 244.0,\n",
       " 178.0,\n",
       " 246.0,\n",
       " 362.0,\n",
       " 392.0,\n",
       " 252.0,\n",
       " 389.0,\n",
       " 400.0,\n",
       " 283.0,\n",
       " 430.0,\n",
       " 421.0,\n",
       " 406.0,\n",
       " 95.0,\n",
       " 377.0,\n",
       " 176.0,\n",
       " 452.0,\n",
       " 49.0,\n",
       " 632.0,\n",
       " 364.0,\n",
       " 436.0,\n",
       " 267.0,\n",
       " 391.0,\n",
       " 1261.0,\n",
       " 303.0,\n",
       " 1.0,\n",
       " 405.0,\n",
       " 434.0,\n",
       " 401.0,\n",
       " 337.0,\n",
       " 463.0,\n",
       " 245.0,\n",
       " 837.0,\n",
       " 968.0,\n",
       " 1258.0,\n",
       " 284.0,\n",
       " 416.0,\n",
       " 835.0,\n",
       " 247.0,\n",
       " 1257.0,\n",
       " 456.0,\n",
       " 1256.0,\n",
       " 431.0,\n",
       " 184.0,\n",
       " 187.0,\n",
       " 455.0,\n",
       " 1255.0,\n",
       " 1203.0,\n",
       " 353.0,\n",
       " 196.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_2147=[]\n",
    "for i in range(162):\n",
    "    for j in proba_list[i]:\n",
    "        res_2147.append(j)\n",
    "res_2147_list=[]\n",
    "for i in range(1296):\n",
    "    cur=[]\n",
    "    cur.append(res_2147[i])\n",
    "    cur.append(i)\n",
    "    res_2147_list.append(cur)\n",
    "res_2147_list\n",
    "res_2147_list=np.array(res_2147_list)\n",
    "sorted_index = sorted(list(range(len(res_2147_list))), key=lambda i : res_2147_list[i].tolist())\n",
    "res_2147_list=res_2147_list[sorted_index]\n",
    "res_2147_list\n",
    "top_50_2147=[]\n",
    "for i in range(50):\n",
    "    top_50_2147.append(res_2147_list[1295-i][1])\n",
    "top_50_2147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hired-oasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[362.0,\n",
       " 240.0,\n",
       " 364.0,\n",
       " 246.0,\n",
       " 244.0,\n",
       " 178.0,\n",
       " 252.0,\n",
       " 389.0,\n",
       " 202.0,\n",
       " 95.0,\n",
       " 49.0,\n",
       " 219.0,\n",
       " 230.0,\n",
       " 391.0,\n",
       " 392.0,\n",
       " 196.0,\n",
       " 561.0,\n",
       " 218.0,\n",
       " 337.0,\n",
       " 563.0,\n",
       " 412.0,\n",
       " 409.0,\n",
       " 552.0,\n",
       " 377.0,\n",
       " 313.0,\n",
       " 206.0,\n",
       " 94.0,\n",
       " 303.0,\n",
       " 204.0,\n",
       " 89.0,\n",
       " 436.0,\n",
       " 65.0,\n",
       " 763.0,\n",
       " 434.0,\n",
       " 212.0,\n",
       " 414.0,\n",
       " 837.0,\n",
       " 400.0,\n",
       " 310.0,\n",
       " 340.0,\n",
       " 1.0,\n",
       " 200.0,\n",
       " 209.0,\n",
       " 370.0,\n",
       " 353.0,\n",
       " 452.0,\n",
       " 430.0,\n",
       " 351.0,\n",
       " 358.0,\n",
       " 538.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_2142=[]\n",
    "for i in range(162):\n",
    "    for j in proba_list[i]:\n",
    "        res_2142.append(j)\n",
    "res_2142_list=[]\n",
    "for i in range(1296):\n",
    "    cur=[]\n",
    "    cur.append(res_2142[i])\n",
    "    cur.append(i)\n",
    "    res_2142_list.append(cur)\n",
    "res_2142_list\n",
    "res_2142_list=np.array(res_2142_list)\n",
    "sorted_index = sorted(list(range(len(res_2142_list))), key=lambda i : res_2142_list[i].tolist())\n",
    "res_2142_list=res_2142_list[sorted_index]\n",
    "res_2142_list\n",
    "top_50_2142=[]\n",
    "for i in range(50):\n",
    "    top_50_2142.append(res_2142_list[1295-i][1])\n",
    "top_50_2142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "breathing-campbell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -2.45583487, 1288.        ],\n",
       "       [  -2.26642895,  394.        ],\n",
       "       [  -2.2421236 ,  418.        ],\n",
       "       ...,\n",
       "       [   3.4009707 ,  364.        ],\n",
       "       [   3.52465439,  240.        ],\n",
       "       [   3.91114902,  362.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_2142_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-spine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:workspace] *",
   "language": "python",
   "name": "conda-env-workspace-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
